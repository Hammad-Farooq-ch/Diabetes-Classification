{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('diabetes.csv')\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,0:8]\n",
    "x=x.values\n",
    "y=data[\"Outcome\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tarin,x_test, y_train, y_test =train_test_split(x,y,test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_model=RandomForestClassifier(n_estimators=200)\n",
    "RF_model.fit(x_tarin, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7864583333333334\n",
      "0.7537999293036408\n"
     ]
    }
   ],
   "source": [
    "y_prediction=RF_model.predict(x_test)\n",
    "accuracy=accuracy_score(y_test,y_prediction)\n",
    "roc=roc_auc_score(y_test,y_prediction)\n",
    "print(accuracy)\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roc_auc_score is an evaluation metric for binary classifier. higher the value, the better the performace of thee model.\n",
    "\n",
    "so, the **accuracy 0.786** and **roc 0.753** will be our benchmark value to evaluate the feed forward neral netork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the follwing cell we will develop and evaluate feed_farword neural networks. we will creat MLP and deep neural nwtwork model and play with there parameters and compare the accuracy with randomForestClassifier. \n",
    "For this task we are going to use **keras** library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=Sequential()\n",
    "model_1.add(Dense(12,input_shape=(8,),activation='sigmoid')) #hidden layer 1 with 12 neuron, \n",
    "              #   and input layer with 8 neuron as 8 feature column we have in dataset\n",
    "model_1.add(Dense(1,activation='sigmoid')) # output layer  only 1 neuron (one output, binary 0 or 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model_1, I use **sigmoid** acivation function,{ domain(-inf to +inf), range(0 to 1). sigmoid activation function is prone to vanishing gradient problem, becsue there is no significent change in output of sigmoid funtion for higher value of input. So, no much change make gradient equal to zero and even vanish at some time.\n",
    "\n",
    "To overcome this vanishing problem in next model_2 we will use **Rectifier Linear Unit (ReLU)**\n",
    "\n",
    "it does not mean that this sigmoid activation function will not perform well all time. we compare its results with other activaion function to observe the difference in performance \n",
    ")\n",
    "there are other activation function also avaible to explore like  **Tanh** and **\"Leaky\" Rectified Linear Unit (LReLU )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(SGD(lr=0.003),\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as in above cell, to compile the model we have to use Keras model API method name as compile. \n",
    "\n",
    "Model.compile(\n",
    "    optimizer=\"SGD\",\n",
    "    loss=None,\n",
    "    metrics=None,\n",
    "    loss_weights=None,\n",
    "    weighted_metrics=None,\n",
    "    run_eagerly=None,\n",
    "    steps_per_execution=None,\n",
    "    **kwargs\n",
    ")\n",
    "in this method we must need to define the optimization method. here we can give string name of optmizer from keras optimizer layer. or we can give a optimizer instant like I have done in above cell. \n",
    "\n",
    "Here I used **stochastic gradient descent SGD** optimizer. you can define it in separate line or inside the model compile mehtod. \n",
    "\n",
    "keras.optimizers.SGD(\n",
    "    learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\", **kwargs\n",
    ")\n",
    "\n",
    "we can set learning rate in optimization mehtod. stochastic gradient descent mean weights will update base on single observation. you set it to full batch gradient or mini batch gradient by seting the batch_size in **model.fit** method. if it is not specified than default size will be 32, that means, it will run mini{batch gradient descent.you can make it 1 for pure stochastic descent or equal to total number of observation for full batch gradient descent.\n",
    "\n",
    "the basic formula to find the optimal weight is same for all optimizer as given below. all other techniques alter this method to find more efficent results\n",
    "\n",
    "                      ** w = w-alpha*gradient**\n",
    "                      \n",
    " if you set the momentum value >0 it will beocme **Momentum Optimizer**. and if you set the nesterov= True it will become the **Nesterov Momentum\n",
    " \n",
    " to set it pure stochastic or full batch or mini batch gradient, change batch_size in fit function\n",
    " \n",
    " Model.fit(\n",
    "    x=None,\n",
    "    y=None,\n",
    "    **batch_size=None**,\n",
    "    epochs=1,\n",
    "    verbose=\"auto\",\n",
    "    callbacks=None,\n",
    "    validation_split=0.0,\n",
    "    validation_data=None,\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    validation_batch_size=None,\n",
    "    validation_freq=1,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "\n",
    "\n",
    "there are many more optimization techniques are available for example, **Ada Grade**, **RMSprop**, **Adam** etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer= StandardScaler()\n",
    "x_train_normalize=normalizer.fit_transform(x_tarin)\n",
    "x_test_normalize=normalizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent update weight base on the scale of the input. so all the input feature should be on one scale to avoid the biasness. one scale mean normalize the all input features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.8918 - accuracy: 0.3455 - val_loss: 0.8847 - val_accuracy: 0.3646\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.8677 - accuracy: 0.3455 - val_loss: 0.8624 - val_accuracy: 0.3646\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.8459 - accuracy: 0.3455 - val_loss: 0.8421 - val_accuracy: 0.3698\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.8260 - accuracy: 0.3403 - val_loss: 0.8237 - val_accuracy: 0.3542\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.8080 - accuracy: 0.3420 - val_loss: 0.8070 - val_accuracy: 0.3542\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7932 - accuracy: 0.37 - 0s 2ms/step - loss: 0.7917 - accuracy: 0.3472 - val_loss: 0.7920 - val_accuracy: 0.3490\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7771 - accuracy: 0.3507 - val_loss: 0.7784 - val_accuracy: 0.3385\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7637 - accuracy: 0.3524 - val_loss: 0.7661 - val_accuracy: 0.3385\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7517 - accuracy: 0.3715 - val_loss: 0.7550 - val_accuracy: 0.3229\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7408 - accuracy: 0.3854 - val_loss: 0.7449 - val_accuracy: 0.3646\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7310 - accuracy: 0.4132 - val_loss: 0.7358 - val_accuracy: 0.3750\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7221 - accuracy: 0.4115 - val_loss: 0.7276 - val_accuracy: 0.3906\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7141 - accuracy: 0.4392 - val_loss: 0.7201 - val_accuracy: 0.4271\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.7068 - accuracy: 0.4514 - val_loss: 0.7134 - val_accuracy: 0.4375\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.4722 - val_loss: 0.7072 - val_accuracy: 0.4583\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.4965 - val_loss: 0.7016 - val_accuracy: 0.4948\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6887 - accuracy: 0.5243 - val_loss: 0.6965 - val_accuracy: 0.5156\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6837 - accuracy: 0.5382 - val_loss: 0.6919 - val_accuracy: 0.5104\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.5712 - val_loss: 0.6876 - val_accuracy: 0.5417\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.6749 - accuracy: 0.5938 - val_loss: 0.6837 - val_accuracy: 0.5729\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6711 - accuracy: 0.6059 - val_loss: 0.6801 - val_accuracy: 0.5885\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6677 - accuracy: 0.6215 - val_loss: 0.6768 - val_accuracy: 0.6094\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.6389 - val_loss: 0.6738 - val_accuracy: 0.6198\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6614 - accuracy: 0.6458 - val_loss: 0.6709 - val_accuracy: 0.6354\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6586 - accuracy: 0.6510 - val_loss: 0.6683 - val_accuracy: 0.6458\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6561 - accuracy: 0.6510 - val_loss: 0.6659 - val_accuracy: 0.6458\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6537 - accuracy: 0.6545 - val_loss: 0.6636 - val_accuracy: 0.6406\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6515 - accuracy: 0.6562 - val_loss: 0.6615 - val_accuracy: 0.6354\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6494 - accuracy: 0.6580 - val_loss: 0.6595 - val_accuracy: 0.6458\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6475 - accuracy: 0.6580 - val_loss: 0.6576 - val_accuracy: 0.6562\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6457 - accuracy: 0.6562 - val_loss: 0.6558 - val_accuracy: 0.6562\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.6615 - val_loss: 0.6541 - val_accuracy: 0.6510\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6423 - accuracy: 0.6649 - val_loss: 0.6525 - val_accuracy: 0.6562\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6408 - accuracy: 0.6632 - val_loss: 0.6510 - val_accuracy: 0.6562\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6393 - accuracy: 0.6615 - val_loss: 0.6496 - val_accuracy: 0.6458\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6379 - accuracy: 0.6632 - val_loss: 0.6482 - val_accuracy: 0.6458\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6366 - accuracy: 0.6632 - val_loss: 0.6468 - val_accuracy: 0.6510\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6353 - accuracy: 0.6615 - val_loss: 0.6456 - val_accuracy: 0.6510\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6341 - accuracy: 0.6632 - val_loss: 0.6443 - val_accuracy: 0.6510\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6329 - accuracy: 0.6632 - val_loss: 0.6431 - val_accuracy: 0.6510\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6318 - accuracy: 0.6632 - val_loss: 0.6420 - val_accuracy: 0.6458\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6307 - accuracy: 0.6649 - val_loss: 0.6408 - val_accuracy: 0.6458\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6297 - accuracy: 0.6667 - val_loss: 0.6398 - val_accuracy: 0.6458\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6286 - accuracy: 0.6667 - val_loss: 0.6387 - val_accuracy: 0.6458\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6276 - accuracy: 0.6649 - val_loss: 0.6376 - val_accuracy: 0.6458\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6267 - accuracy: 0.6649 - val_loss: 0.6366 - val_accuracy: 0.6458\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6257 - accuracy: 0.6649 - val_loss: 0.6356 - val_accuracy: 0.6458\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6248 - accuracy: 0.6632 - val_loss: 0.6347 - val_accuracy: 0.6510\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.6632 - val_loss: 0.6337 - val_accuracy: 0.6510\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6230 - accuracy: 0.6632 - val_loss: 0.6328 - val_accuracy: 0.6510\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.6632 - val_loss: 0.6318 - val_accuracy: 0.6510\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6213 - accuracy: 0.6632 - val_loss: 0.6309 - val_accuracy: 0.6510\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6204 - accuracy: 0.6632 - val_loss: 0.6300 - val_accuracy: 0.6510\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6196 - accuracy: 0.6632 - val_loss: 0.6291 - val_accuracy: 0.6510\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6188 - accuracy: 0.6632 - val_loss: 0.6283 - val_accuracy: 0.6510\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6180 - accuracy: 0.6632 - val_loss: 0.6274 - val_accuracy: 0.6510\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.6632 - val_loss: 0.6266 - val_accuracy: 0.6510\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6165 - accuracy: 0.6632 - val_loss: 0.6257 - val_accuracy: 0.6510\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6157 - accuracy: 0.6632 - val_loss: 0.6249 - val_accuracy: 0.6510\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6149 - accuracy: 0.6632 - val_loss: 0.6240 - val_accuracy: 0.6510\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6141 - accuracy: 0.6632 - val_loss: 0.6232 - val_accuracy: 0.6510\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6134 - accuracy: 0.6632 - val_loss: 0.6224 - val_accuracy: 0.6510\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.6632 - val_loss: 0.6216 - val_accuracy: 0.6510\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6120 - accuracy: 0.6632 - val_loss: 0.6208 - val_accuracy: 0.6510\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6112 - accuracy: 0.6632 - val_loss: 0.6200 - val_accuracy: 0.6510\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6105 - accuracy: 0.6632 - val_loss: 0.6192 - val_accuracy: 0.6510\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.6632 - val_loss: 0.6184 - val_accuracy: 0.6510\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6091 - accuracy: 0.6632 - val_loss: 0.6177 - val_accuracy: 0.6510\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6084 - accuracy: 0.6632 - val_loss: 0.6169 - val_accuracy: 0.6510\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.6632 - val_loss: 0.6161 - val_accuracy: 0.6510\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6070 - accuracy: 0.6632 - val_loss: 0.6154 - val_accuracy: 0.6510\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6064 - accuracy: 0.6632 - val_loss: 0.6146 - val_accuracy: 0.6510\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6057 - accuracy: 0.6615 - val_loss: 0.6139 - val_accuracy: 0.6510\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6050 - accuracy: 0.6632 - val_loss: 0.6131 - val_accuracy: 0.6510\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.6632 - val_loss: 0.6124 - val_accuracy: 0.6510\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6037 - accuracy: 0.6632 - val_loss: 0.6117 - val_accuracy: 0.6510\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6030 - accuracy: 0.6632 - val_loss: 0.6109 - val_accuracy: 0.6510\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6024 - accuracy: 0.6632 - val_loss: 0.6102 - val_accuracy: 0.6510\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6017 - accuracy: 0.6632 - val_loss: 0.6095 - val_accuracy: 0.6510\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6010 - accuracy: 0.6632 - val_loss: 0.6088 - val_accuracy: 0.6510\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.6632 - val_loss: 0.6080 - val_accuracy: 0.6510\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5998 - accuracy: 0.6667 - val_loss: 0.6073 - val_accuracy: 0.6510\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5992 - accuracy: 0.6684 - val_loss: 0.6066 - val_accuracy: 0.6510\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5985 - accuracy: 0.6701 - val_loss: 0.6059 - val_accuracy: 0.6510\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.6701 - val_loss: 0.6052 - val_accuracy: 0.6510\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.6719 - val_loss: 0.6045 - val_accuracy: 0.6510\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5967 - accuracy: 0.6719 - val_loss: 0.6038 - val_accuracy: 0.6510\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.6736 - val_loss: 0.6031 - val_accuracy: 0.6510\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5954 - accuracy: 0.6736 - val_loss: 0.6025 - val_accuracy: 0.6562\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.6736 - val_loss: 0.6018 - val_accuracy: 0.6562\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5942 - accuracy: 0.6753 - val_loss: 0.6011 - val_accuracy: 0.6615\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.6753 - val_loss: 0.6004 - val_accuracy: 0.6615\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.6753 - val_loss: 0.5998 - val_accuracy: 0.6615\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5924 - accuracy: 0.6771 - val_loss: 0.5991 - val_accuracy: 0.6615\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5918 - accuracy: 0.6753 - val_loss: 0.5984 - val_accuracy: 0.6667\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5912 - accuracy: 0.6753 - val_loss: 0.5978 - val_accuracy: 0.6719\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5906 - accuracy: 0.6753 - val_loss: 0.5971 - val_accuracy: 0.6719\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5901 - accuracy: 0.6771 - val_loss: 0.5965 - val_accuracy: 0.6719\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.6788 - val_loss: 0.5958 - val_accuracy: 0.6719\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5889 - accuracy: 0.6788 - val_loss: 0.5952 - val_accuracy: 0.6719\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.6806 - val_loss: 0.5945 - val_accuracy: 0.6719\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5878 - accuracy: 0.6806 - val_loss: 0.5939 - val_accuracy: 0.6719\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.6806 - val_loss: 0.5932 - val_accuracy: 0.6719\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5866 - accuracy: 0.6806 - val_loss: 0.5926 - val_accuracy: 0.6771\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5861 - accuracy: 0.6806 - val_loss: 0.5920 - val_accuracy: 0.6771\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5855 - accuracy: 0.6823 - val_loss: 0.5913 - val_accuracy: 0.6771\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.6823 - val_loss: 0.5907 - val_accuracy: 0.6771\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.6823 - val_loss: 0.5901 - val_accuracy: 0.6823\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.6823 - val_loss: 0.5895 - val_accuracy: 0.6875\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5832 - accuracy: 0.6806 - val_loss: 0.5889 - val_accuracy: 0.6875\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5827 - accuracy: 0.6806 - val_loss: 0.5882 - val_accuracy: 0.6875\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5822 - accuracy: 0.6823 - val_loss: 0.5876 - val_accuracy: 0.6875\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.6823 - val_loss: 0.5870 - val_accuracy: 0.6875\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.6823 - val_loss: 0.5864 - val_accuracy: 0.6875\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5805 - accuracy: 0.6823 - val_loss: 0.5858 - val_accuracy: 0.6875\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5800 - accuracy: 0.6840 - val_loss: 0.5852 - val_accuracy: 0.6875\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5795 - accuracy: 0.6840 - val_loss: 0.5846 - val_accuracy: 0.6875\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5789 - accuracy: 0.6858 - val_loss: 0.5840 - val_accuracy: 0.6875\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5785 - accuracy: 0.6858 - val_loss: 0.5834 - val_accuracy: 0.6875\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5779 - accuracy: 0.6875 - val_loss: 0.5828 - val_accuracy: 0.6875\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5774 - accuracy: 0.6875 - val_loss: 0.5823 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5769 - accuracy: 0.6875 - val_loss: 0.5817 - val_accuracy: 0.6875\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.6875 - val_loss: 0.5811 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5758 - accuracy: 0.6875 - val_loss: 0.5805 - val_accuracy: 0.6875\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5753 - accuracy: 0.6875 - val_loss: 0.5799 - val_accuracy: 0.6875\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5748 - accuracy: 0.6875 - val_loss: 0.5794 - val_accuracy: 0.6927\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5743 - accuracy: 0.6875 - val_loss: 0.5788 - val_accuracy: 0.6927\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5738 - accuracy: 0.6892 - val_loss: 0.5782 - val_accuracy: 0.6927\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5733 - accuracy: 0.6892 - val_loss: 0.5777 - val_accuracy: 0.6979\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5728 - accuracy: 0.6892 - val_loss: 0.5771 - val_accuracy: 0.6979\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5723 - accuracy: 0.6892 - val_loss: 0.5766 - val_accuracy: 0.6979\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5718 - accuracy: 0.6892 - val_loss: 0.5760 - val_accuracy: 0.6979\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.6892 - val_loss: 0.5754 - val_accuracy: 0.7031\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5708 - accuracy: 0.6927 - val_loss: 0.5749 - val_accuracy: 0.7031\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.6927 - val_loss: 0.5743 - val_accuracy: 0.7031\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.6910 - val_loss: 0.5738 - val_accuracy: 0.7031\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5694 - accuracy: 0.6927 - val_loss: 0.5733 - val_accuracy: 0.7031\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.6927 - val_loss: 0.5727 - val_accuracy: 0.6979\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.6910 - val_loss: 0.5722 - val_accuracy: 0.6927\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.6910 - val_loss: 0.5716 - val_accuracy: 0.6927\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.6910 - val_loss: 0.5711 - val_accuracy: 0.6927\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.6927 - val_loss: 0.5706 - val_accuracy: 0.6927\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.6927 - val_loss: 0.5701 - val_accuracy: 0.6927\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5660 - accuracy: 0.6927 - val_loss: 0.5695 - val_accuracy: 0.6927\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.6927 - val_loss: 0.5690 - val_accuracy: 0.6979\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.6927 - val_loss: 0.5685 - val_accuracy: 0.6979\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.6927 - val_loss: 0.5680 - val_accuracy: 0.7031\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5642 - accuracy: 0.6927 - val_loss: 0.5674 - val_accuracy: 0.7031\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5637 - accuracy: 0.6944 - val_loss: 0.5669 - val_accuracy: 0.7031\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5632 - accuracy: 0.6944 - val_loss: 0.5664 - val_accuracy: 0.7031\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.6962 - val_loss: 0.5659 - val_accuracy: 0.7031\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.6962 - val_loss: 0.5654 - val_accuracy: 0.7031\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5619 - accuracy: 0.6979 - val_loss: 0.5649 - val_accuracy: 0.7031\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5615 - accuracy: 0.6979 - val_loss: 0.5644 - val_accuracy: 0.6979\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.6979 - val_loss: 0.5639 - val_accuracy: 0.7031\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.6979 - val_loss: 0.5634 - val_accuracy: 0.7031\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.6997 - val_loss: 0.5629 - val_accuracy: 0.7031\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5597 - accuracy: 0.6962 - val_loss: 0.5624 - val_accuracy: 0.7031\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.6962 - val_loss: 0.5619 - val_accuracy: 0.6927\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.6962 - val_loss: 0.5614 - val_accuracy: 0.6979\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5584 - accuracy: 0.6962 - val_loss: 0.5610 - val_accuracy: 0.6979\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5579 - accuracy: 0.6962 - val_loss: 0.5605 - val_accuracy: 0.6979\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.6944 - val_loss: 0.5600 - val_accuracy: 0.6979\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5571 - accuracy: 0.6944 - val_loss: 0.5595 - val_accuracy: 0.6979\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5566 - accuracy: 0.6944 - val_loss: 0.5590 - val_accuracy: 0.6979\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5562 - accuracy: 0.6944 - val_loss: 0.5586 - val_accuracy: 0.6979\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5558 - accuracy: 0.6944 - val_loss: 0.5581 - val_accuracy: 0.6979\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5554 - accuracy: 0.6944 - val_loss: 0.5576 - val_accuracy: 0.7031\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.78 - 0s 1ms/step - loss: 0.5550 - accuracy: 0.6962 - val_loss: 0.5572 - val_accuracy: 0.7031\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5546 - accuracy: 0.6962 - val_loss: 0.5567 - val_accuracy: 0.7031\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.6997 - val_loss: 0.5562 - val_accuracy: 0.7083\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7014 - val_loss: 0.5558 - val_accuracy: 0.7083\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5533 - accuracy: 0.6997 - val_loss: 0.5553 - val_accuracy: 0.7083\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.7014 - val_loss: 0.5548 - val_accuracy: 0.7083\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5525 - accuracy: 0.7031 - val_loss: 0.5544 - val_accuracy: 0.7083\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5521 - accuracy: 0.7031 - val_loss: 0.5539 - val_accuracy: 0.7083\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5517 - accuracy: 0.7031 - val_loss: 0.5535 - val_accuracy: 0.7083\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5513 - accuracy: 0.7031 - val_loss: 0.5530 - val_accuracy: 0.7083\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5509 - accuracy: 0.7066 - val_loss: 0.5526 - val_accuracy: 0.7083\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7066 - val_loss: 0.5521 - val_accuracy: 0.7135\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5501 - accuracy: 0.7083 - val_loss: 0.5517 - val_accuracy: 0.7188\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5497 - accuracy: 0.7083 - val_loss: 0.5513 - val_accuracy: 0.7188\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.7066 - val_loss: 0.5508 - val_accuracy: 0.7188\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5489 - accuracy: 0.7083 - val_loss: 0.5504 - val_accuracy: 0.7240\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5485 - accuracy: 0.7083 - val_loss: 0.5499 - val_accuracy: 0.7240\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5481 - accuracy: 0.7083 - val_loss: 0.5495 - val_accuracy: 0.7240\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.7083 - val_loss: 0.5491 - val_accuracy: 0.7240\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5473 - accuracy: 0.7083 - val_loss: 0.5487 - val_accuracy: 0.7240\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5470 - accuracy: 0.7083 - val_loss: 0.5482 - val_accuracy: 0.7292\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5466 - accuracy: 0.7101 - val_loss: 0.5478 - val_accuracy: 0.7292\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7118 - val_loss: 0.5474 - val_accuracy: 0.7292\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5458 - accuracy: 0.7135 - val_loss: 0.5470 - val_accuracy: 0.7292\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5455 - accuracy: 0.7135 - val_loss: 0.5465 - val_accuracy: 0.7292\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7135 - val_loss: 0.5461 - val_accuracy: 0.7292\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5447 - accuracy: 0.7135 - val_loss: 0.5457 - val_accuracy: 0.7292\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.7118 - val_loss: 0.5453 - val_accuracy: 0.7292\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7118 - val_loss: 0.5449 - val_accuracy: 0.7292\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5436 - accuracy: 0.7153 - val_loss: 0.5445 - val_accuracy: 0.7292\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.7188 - val_loss: 0.5441 - val_accuracy: 0.7292\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5429 - accuracy: 0.7188 - val_loss: 0.5437 - val_accuracy: 0.7344\n"
     ]
    }
   ],
   "source": [
    "develop_model_1_history=model_1.fit(x_train_normalize,y_train,validation_data=(x_test_normalize,y_test), epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch mean a single run over all data set. for example in mini_batch gradient , let's suppose total observatio are 320 and mini_batch size is 32 so 10 steps will be taken to run over all data observation therefore 1 epoch will be equal to 10 step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734375\n",
      "0.6527041357370096\n"
     ]
    }
   ],
   "source": [
    "y_predcit_from_model_1=model_1.predict_classes(x_test_normalize)\n",
    "model_1_accuracy=accuracy_score(y_test,y_predcit_from_model_1)\n",
    "model_1_roc=roc_auc_score(y_test,y_predcit_from_model_1)\n",
    "print(model_1_accuracy)\n",
    "print(model_1_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest model accuracy was 0.786 and neural netwrok of one hiddent layer with 12 neuron has acuraccy 0.73. so far, we cam observe that randomForest perform better than NN in this particular case  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "develop_model_1_history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1426be259a0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(develop_model_1_history.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "plt.plot(develop_model_1_history.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If validation loss is more than traing loss than its mean your model is overfitting. in above plot both losses are almost same at 200 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--** in below cells i will make a function that can train to neral network with differnet activation function and optimizer tachniques. function we help to reuse the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1_x(activation_function,optimizer_technique): # in this function activation function and optimization technique will\n",
    "                                                        #be pass as function argument\n",
    "    model=Sequential()\n",
    "    model.add(Dense(12,input_shape=(8,),activation=activation_function)) #hidden layer 1 with 12 neuron, \n",
    "              #   and input layer with 8 neuron as 8 feature column we have in dataset\n",
    "    model.add(Dense(1,activation='sigmoid')) # output layer  only 1 neuron (one output, binary 0 or 1) \n",
    "    model.compile(optimizer=optimizer_technique,loss=\"binary_crossentropy\",metrics=[\"accuracy\"]);\n",
    "    develop_model_history=model.fit(x_train_normalize,y_train,validation_data=(x_test_normalize,y_test), epochs=200);\n",
    "    \n",
    "    y_predcit_from_model=model.predict_classes(x_test_normalize)\n",
    "    \n",
    "    model_accuracy=accuracy_score(y_test,y_predcit_from_model)\n",
    "    model_roc=roc_auc_score(y_test,y_predcit_from_model)\n",
    "    print(\"accuracy is = \",model_1_accuracy)\n",
    "    print(\"ROC is = \",model_1_roc)\n",
    "    \n",
    "    plt.plot(develop_model_history.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "    plt.plot(develop_model_history.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.7134 - accuracy: 0.5174 - val_loss: 0.7084 - val_accuracy: 0.5260\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.7049 - accuracy: 0.5712 - val_loss: 0.7016 - val_accuracy: 0.5729\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6988 - accuracy: 0.5833 - val_loss: 0.6963 - val_accuracy: 0.5833\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6941 - accuracy: 0.5885 - val_loss: 0.6920 - val_accuracy: 0.5833\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5955 - val_loss: 0.6882 - val_accuracy: 0.5781\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6864 - accuracy: 0.6007 - val_loss: 0.6849 - val_accuracy: 0.5781\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6833 - accuracy: 0.6111 - val_loss: 0.6819 - val_accuracy: 0.5781\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6804 - accuracy: 0.6181 - val_loss: 0.6792 - val_accuracy: 0.5781\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6779 - accuracy: 0.6233 - val_loss: 0.6767 - val_accuracy: 0.5885\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6755 - accuracy: 0.6267 - val_loss: 0.6743 - val_accuracy: 0.5938\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6733 - accuracy: 0.6354 - val_loss: 0.6721 - val_accuracy: 0.5990\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6712 - accuracy: 0.6372 - val_loss: 0.6701 - val_accuracy: 0.6094\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6693 - accuracy: 0.6389 - val_loss: 0.6682 - val_accuracy: 0.6146\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6674 - accuracy: 0.6406 - val_loss: 0.6663 - val_accuracy: 0.6146\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6657 - accuracy: 0.6458 - val_loss: 0.6646 - val_accuracy: 0.6042\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6640 - accuracy: 0.6476 - val_loss: 0.6629 - val_accuracy: 0.6094\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6625 - accuracy: 0.6441 - val_loss: 0.6613 - val_accuracy: 0.6146\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6610 - accuracy: 0.6458 - val_loss: 0.6598 - val_accuracy: 0.6146\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6595 - accuracy: 0.6493 - val_loss: 0.6583 - val_accuracy: 0.6146\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6581 - accuracy: 0.6493 - val_loss: 0.6569 - val_accuracy: 0.6250\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6568 - accuracy: 0.6493 - val_loss: 0.6555 - val_accuracy: 0.6250\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6555 - accuracy: 0.6510 - val_loss: 0.6542 - val_accuracy: 0.6198\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.6510 - val_loss: 0.6529 - val_accuracy: 0.6198\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.6510 - val_loss: 0.6517 - val_accuracy: 0.6146\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6519 - accuracy: 0.6528 - val_loss: 0.6505 - val_accuracy: 0.6146\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.6545 - val_loss: 0.6493 - val_accuracy: 0.6094\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6497 - accuracy: 0.6562 - val_loss: 0.6482 - val_accuracy: 0.6094\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.6597 - val_loss: 0.6471 - val_accuracy: 0.6146\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.6597 - val_loss: 0.6460 - val_accuracy: 0.6302\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6465 - accuracy: 0.6615 - val_loss: 0.6449 - val_accuracy: 0.6302\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6456 - accuracy: 0.6615 - val_loss: 0.6439 - val_accuracy: 0.6302\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6446 - accuracy: 0.6615 - val_loss: 0.6429 - val_accuracy: 0.6302\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.6632 - val_loss: 0.6419 - val_accuracy: 0.6302\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6428 - accuracy: 0.6632 - val_loss: 0.6410 - val_accuracy: 0.6354\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6419 - accuracy: 0.6632 - val_loss: 0.6400 - val_accuracy: 0.6354\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6410 - accuracy: 0.6632 - val_loss: 0.6391 - val_accuracy: 0.6354\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.6649 - val_loss: 0.6382 - val_accuracy: 0.6354\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6393 - accuracy: 0.6649 - val_loss: 0.6373 - val_accuracy: 0.6354\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6649 - val_loss: 0.6365 - val_accuracy: 0.6354\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.6649 - val_loss: 0.6356 - val_accuracy: 0.6354\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6369 - accuracy: 0.6649 - val_loss: 0.6348 - val_accuracy: 0.6354\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6361 - accuracy: 0.6667 - val_loss: 0.6340 - val_accuracy: 0.6302\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6667 - val_loss: 0.6332 - val_accuracy: 0.6302\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6346 - accuracy: 0.6684 - val_loss: 0.6324 - val_accuracy: 0.6302\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6339 - accuracy: 0.6684 - val_loss: 0.6316 - val_accuracy: 0.6302\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6332 - accuracy: 0.6684 - val_loss: 0.6309 - val_accuracy: 0.6302\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6325 - accuracy: 0.6684 - val_loss: 0.6301 - val_accuracy: 0.6302\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6318 - accuracy: 0.6684 - val_loss: 0.6294 - val_accuracy: 0.6354\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6311 - accuracy: 0.6684 - val_loss: 0.6287 - val_accuracy: 0.6354\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6304 - accuracy: 0.6684 - val_loss: 0.6279 - val_accuracy: 0.6406\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6298 - accuracy: 0.6684 - val_loss: 0.6272 - val_accuracy: 0.6406\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6291 - accuracy: 0.6684 - val_loss: 0.6265 - val_accuracy: 0.6458\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6284 - accuracy: 0.6684 - val_loss: 0.6259 - val_accuracy: 0.6458\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6278 - accuracy: 0.6684 - val_loss: 0.6252 - val_accuracy: 0.6510\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6272 - accuracy: 0.6684 - val_loss: 0.6245 - val_accuracy: 0.6510\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6266 - accuracy: 0.6701 - val_loss: 0.6239 - val_accuracy: 0.6510\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6260 - accuracy: 0.6701 - val_loss: 0.6232 - val_accuracy: 0.6510\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6254 - accuracy: 0.6736 - val_loss: 0.6226 - val_accuracy: 0.6510\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6248 - accuracy: 0.6736 - val_loss: 0.6219 - val_accuracy: 0.6510\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.6719 - val_loss: 0.6213 - val_accuracy: 0.6510\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.6719 - val_loss: 0.6207 - val_accuracy: 0.6510\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6231 - accuracy: 0.6736 - val_loss: 0.6201 - val_accuracy: 0.6510\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6225 - accuracy: 0.6736 - val_loss: 0.6195 - val_accuracy: 0.6458\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6219 - accuracy: 0.6736 - val_loss: 0.6189 - val_accuracy: 0.6458\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6214 - accuracy: 0.6736 - val_loss: 0.6183 - val_accuracy: 0.6458\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6209 - accuracy: 0.6736 - val_loss: 0.6177 - val_accuracy: 0.6406\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6203 - accuracy: 0.6736 - val_loss: 0.6172 - val_accuracy: 0.6406\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6198 - accuracy: 0.6736 - val_loss: 0.6166 - val_accuracy: 0.6406\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6193 - accuracy: 0.6736 - val_loss: 0.6160 - val_accuracy: 0.6406\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6188 - accuracy: 0.6736 - val_loss: 0.6155 - val_accuracy: 0.6406\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.6736 - val_loss: 0.6150 - val_accuracy: 0.6406\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.6736 - val_loss: 0.6144 - val_accuracy: 0.6406\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6173 - accuracy: 0.6736 - val_loss: 0.6139 - val_accuracy: 0.6406\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.6753 - val_loss: 0.6134 - val_accuracy: 0.6458\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6163 - accuracy: 0.6771 - val_loss: 0.6128 - val_accuracy: 0.6406\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6158 - accuracy: 0.6771 - val_loss: 0.6123 - val_accuracy: 0.6406\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6153 - accuracy: 0.6771 - val_loss: 0.6118 - val_accuracy: 0.6406\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6148 - accuracy: 0.6771 - val_loss: 0.6113 - val_accuracy: 0.6458\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6144 - accuracy: 0.6788 - val_loss: 0.6108 - val_accuracy: 0.6458\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6139 - accuracy: 0.6788 - val_loss: 0.6103 - val_accuracy: 0.6458\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.6788 - val_loss: 0.6098 - val_accuracy: 0.6458\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6130 - accuracy: 0.6806 - val_loss: 0.6093 - val_accuracy: 0.6458\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6125 - accuracy: 0.6806 - val_loss: 0.6088 - val_accuracy: 0.6458\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6121 - accuracy: 0.6806 - val_loss: 0.6084 - val_accuracy: 0.6458\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6117 - accuracy: 0.6806 - val_loss: 0.6079 - val_accuracy: 0.6458\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6112 - accuracy: 0.6806 - val_loss: 0.6074 - val_accuracy: 0.6458\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6108 - accuracy: 0.6806 - val_loss: 0.6070 - val_accuracy: 0.6458\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6104 - accuracy: 0.6823 - val_loss: 0.6065 - val_accuracy: 0.6458\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6099 - accuracy: 0.6806 - val_loss: 0.6060 - val_accuracy: 0.6458\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.6823 - val_loss: 0.6056 - val_accuracy: 0.6458\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6091 - accuracy: 0.6823 - val_loss: 0.6051 - val_accuracy: 0.6458\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6087 - accuracy: 0.6840 - val_loss: 0.6047 - val_accuracy: 0.6458\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6083 - accuracy: 0.6823 - val_loss: 0.6043 - val_accuracy: 0.6458\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6078 - accuracy: 0.6823 - val_loss: 0.6038 - val_accuracy: 0.6458\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6075 - accuracy: 0.6823 - val_loss: 0.6034 - val_accuracy: 0.6510\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6071 - accuracy: 0.6823 - val_loss: 0.6030 - val_accuracy: 0.6510\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.6823 - val_loss: 0.6025 - val_accuracy: 0.6510\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.6823 - val_loss: 0.6021 - val_accuracy: 0.6510\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6059 - accuracy: 0.6806 - val_loss: 0.6017 - val_accuracy: 0.6510\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6055 - accuracy: 0.6823 - val_loss: 0.6013 - val_accuracy: 0.6510\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.6806 - val_loss: 0.6009 - val_accuracy: 0.6510\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6047 - accuracy: 0.6806 - val_loss: 0.6005 - val_accuracy: 0.6510\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6044 - accuracy: 0.6806 - val_loss: 0.6001 - val_accuracy: 0.6510\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6040 - accuracy: 0.6806 - val_loss: 0.5996 - val_accuracy: 0.6510\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6036 - accuracy: 0.6806 - val_loss: 0.5993 - val_accuracy: 0.6510\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6032 - accuracy: 0.6806 - val_loss: 0.5989 - val_accuracy: 0.6510\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6029 - accuracy: 0.6806 - val_loss: 0.5985 - val_accuracy: 0.6562\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6025 - accuracy: 0.6806 - val_loss: 0.5981 - val_accuracy: 0.6562\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.6806 - val_loss: 0.5977 - val_accuracy: 0.6562\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6018 - accuracy: 0.6806 - val_loss: 0.5973 - val_accuracy: 0.6562\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6014 - accuracy: 0.6806 - val_loss: 0.5969 - val_accuracy: 0.6562\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6011 - accuracy: 0.6806 - val_loss: 0.5965 - val_accuracy: 0.6562\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.6788 - val_loss: 0.5962 - val_accuracy: 0.6562\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.6788 - val_loss: 0.5958 - val_accuracy: 0.6562\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6000 - accuracy: 0.6788 - val_loss: 0.5954 - val_accuracy: 0.6562\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5997 - accuracy: 0.6806 - val_loss: 0.5951 - val_accuracy: 0.6562\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5993 - accuracy: 0.6823 - val_loss: 0.5947 - val_accuracy: 0.6562\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5990 - accuracy: 0.6840 - val_loss: 0.5943 - val_accuracy: 0.6615\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.6840 - val_loss: 0.5940 - val_accuracy: 0.6615\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.6840 - val_loss: 0.5936 - val_accuracy: 0.6615\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.6840 - val_loss: 0.5933 - val_accuracy: 0.6615\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5977 - accuracy: 0.6823 - val_loss: 0.5929 - val_accuracy: 0.6615\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.6823 - val_loss: 0.5926 - val_accuracy: 0.6615\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5970 - accuracy: 0.6823 - val_loss: 0.5922 - val_accuracy: 0.6615\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5967 - accuracy: 0.6823 - val_loss: 0.5919 - val_accuracy: 0.6615\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5964 - accuracy: 0.6823 - val_loss: 0.5915 - val_accuracy: 0.6562\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.6823 - val_loss: 0.5912 - val_accuracy: 0.6562\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5957 - accuracy: 0.6823 - val_loss: 0.5908 - val_accuracy: 0.6562\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5954 - accuracy: 0.6823 - val_loss: 0.5905 - val_accuracy: 0.6562\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5951 - accuracy: 0.6823 - val_loss: 0.5902 - val_accuracy: 0.6562\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5948 - accuracy: 0.6823 - val_loss: 0.5898 - val_accuracy: 0.6562\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5945 - accuracy: 0.6840 - val_loss: 0.5895 - val_accuracy: 0.6562\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.6840 - val_loss: 0.5892 - val_accuracy: 0.6562\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5939 - accuracy: 0.6840 - val_loss: 0.5889 - val_accuracy: 0.6562\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.6858 - val_loss: 0.5885 - val_accuracy: 0.6562\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5932 - accuracy: 0.6858 - val_loss: 0.5882 - val_accuracy: 0.6562\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.6858 - val_loss: 0.5879 - val_accuracy: 0.6562\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5926 - accuracy: 0.6875 - val_loss: 0.5876 - val_accuracy: 0.6562\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.6875 - val_loss: 0.5873 - val_accuracy: 0.6562\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.6875 - val_loss: 0.5869 - val_accuracy: 0.6615\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5918 - accuracy: 0.6858 - val_loss: 0.5866 - val_accuracy: 0.6615\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.6875 - val_loss: 0.5863 - val_accuracy: 0.6615\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5912 - accuracy: 0.6875 - val_loss: 0.5860 - val_accuracy: 0.6615\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.6875 - val_loss: 0.5857 - val_accuracy: 0.6615\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.6892 - val_loss: 0.5854 - val_accuracy: 0.6615\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5903 - accuracy: 0.6892 - val_loss: 0.5851 - val_accuracy: 0.6615\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5900 - accuracy: 0.6892 - val_loss: 0.5848 - val_accuracy: 0.6615\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.6892 - val_loss: 0.5845 - val_accuracy: 0.6615\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.6892 - val_loss: 0.5842 - val_accuracy: 0.6615\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.6892 - val_loss: 0.5839 - val_accuracy: 0.6615\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5889 - accuracy: 0.6892 - val_loss: 0.5836 - val_accuracy: 0.6615\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5886 - accuracy: 0.6892 - val_loss: 0.5833 - val_accuracy: 0.6615\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5884 - accuracy: 0.6892 - val_loss: 0.5830 - val_accuracy: 0.6615\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5881 - accuracy: 0.6892 - val_loss: 0.5827 - val_accuracy: 0.6615\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5878 - accuracy: 0.6910 - val_loss: 0.5824 - val_accuracy: 0.6615\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5875 - accuracy: 0.6910 - val_loss: 0.5821 - val_accuracy: 0.6615\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5873 - accuracy: 0.6910 - val_loss: 0.5819 - val_accuracy: 0.6667\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5870 - accuracy: 0.6910 - val_loss: 0.5816 - val_accuracy: 0.6667\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5867 - accuracy: 0.6910 - val_loss: 0.5813 - val_accuracy: 0.6667\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.6910 - val_loss: 0.5810 - val_accuracy: 0.6667\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.6910 - val_loss: 0.5807 - val_accuracy: 0.6667\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5859 - accuracy: 0.6910 - val_loss: 0.5804 - val_accuracy: 0.6667\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5857 - accuracy: 0.6910 - val_loss: 0.5802 - val_accuracy: 0.6667\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.6910 - val_loss: 0.5799 - val_accuracy: 0.6667\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5852 - accuracy: 0.6892 - val_loss: 0.5796 - val_accuracy: 0.6667\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.6892 - val_loss: 0.5793 - val_accuracy: 0.6615\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5846 - accuracy: 0.6892 - val_loss: 0.5791 - val_accuracy: 0.6615\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.6892 - val_loss: 0.5788 - val_accuracy: 0.6615\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5841 - accuracy: 0.6892 - val_loss: 0.5785 - val_accuracy: 0.6615\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5839 - accuracy: 0.6892 - val_loss: 0.5783 - val_accuracy: 0.6615\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5836 - accuracy: 0.6892 - val_loss: 0.5780 - val_accuracy: 0.6615\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5834 - accuracy: 0.6892 - val_loss: 0.5777 - val_accuracy: 0.6615\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5831 - accuracy: 0.6892 - val_loss: 0.5775 - val_accuracy: 0.6615\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5829 - accuracy: 0.6892 - val_loss: 0.5772 - val_accuracy: 0.6615\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.6875 - val_loss: 0.5770 - val_accuracy: 0.6615\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5824 - accuracy: 0.6875 - val_loss: 0.5767 - val_accuracy: 0.6615\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.6892 - val_loss: 0.5764 - val_accuracy: 0.6667\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5819 - accuracy: 0.6892 - val_loss: 0.5762 - val_accuracy: 0.6667\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5817 - accuracy: 0.6892 - val_loss: 0.5759 - val_accuracy: 0.6667\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.6875 - val_loss: 0.5757 - val_accuracy: 0.6667\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5812 - accuracy: 0.6858 - val_loss: 0.5754 - val_accuracy: 0.6667\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.62 - 0s 2ms/step - loss: 0.5809 - accuracy: 0.6875 - val_loss: 0.5752 - val_accuracy: 0.6667\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5807 - accuracy: 0.6875 - val_loss: 0.5749 - val_accuracy: 0.6667\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5805 - accuracy: 0.6875 - val_loss: 0.5747 - val_accuracy: 0.6667\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.6875 - val_loss: 0.5744 - val_accuracy: 0.6667\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5800 - accuracy: 0.6875 - val_loss: 0.5742 - val_accuracy: 0.6667\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5797 - accuracy: 0.6875 - val_loss: 0.5739 - val_accuracy: 0.6667\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.6875 - val_loss: 0.5737 - val_accuracy: 0.6667\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.6875 - val_loss: 0.5734 - val_accuracy: 0.6667\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.6875 - val_loss: 0.5732 - val_accuracy: 0.6667\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.6875 - val_loss: 0.5729 - val_accuracy: 0.6667\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.6875 - val_loss: 0.5727 - val_accuracy: 0.6667\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5784 - accuracy: 0.6892 - val_loss: 0.5725 - val_accuracy: 0.6667\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5781 - accuracy: 0.6875 - val_loss: 0.5722 - val_accuracy: 0.6667\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5779 - accuracy: 0.6875 - val_loss: 0.5720 - val_accuracy: 0.6667\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.6892 - val_loss: 0.5717 - val_accuracy: 0.6667\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.6892 - val_loss: 0.5715 - val_accuracy: 0.6719\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5772 - accuracy: 0.6892 - val_loss: 0.5713 - val_accuracy: 0.6719\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5770 - accuracy: 0.6910 - val_loss: 0.5710 - val_accuracy: 0.6719\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.6910 - val_loss: 0.5708 - val_accuracy: 0.6719\n",
      "accuracy is =  0.734375\n",
      "ROC is =  0.6527041357370096\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation_func=\"sigmoid\"\n",
    "optimizer_tec=Adagrad(learning_rate=0.003)\n",
    "model_1_x(activation_func,optimizer_tec);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6162 - accuracy: 0.6580 - val_loss: 0.6031 - val_accuracy: 0.6406\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5767 - accuracy: 0.6701 - val_loss: 0.5688 - val_accuracy: 0.6823\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5536 - accuracy: 0.6927 - val_loss: 0.5440 - val_accuracy: 0.7135\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7448 - val_loss: 0.5242 - val_accuracy: 0.7656\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7622 - val_loss: 0.5083 - val_accuracy: 0.7917\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7778 - val_loss: 0.4960 - val_accuracy: 0.7708\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7778 - val_loss: 0.4868 - val_accuracy: 0.7812\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7778 - val_loss: 0.4799 - val_accuracy: 0.7760\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.7726 - val_loss: 0.4747 - val_accuracy: 0.7812\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7708 - val_loss: 0.4706 - val_accuracy: 0.7760\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7656 - val_loss: 0.4676 - val_accuracy: 0.7760\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4549 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7708 - val_loss: 0.4646 - val_accuracy: 0.7812\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4751 - accuracy: 0.7691 - val_loss: 0.4627 - val_accuracy: 0.7865\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7639 - val_loss: 0.4616 - val_accuracy: 0.7865\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7743 - val_loss: 0.4604 - val_accuracy: 0.7865\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7674 - val_loss: 0.4597 - val_accuracy: 0.7865\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.7674 - val_loss: 0.4590 - val_accuracy: 0.7812\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.7674 - val_loss: 0.4582 - val_accuracy: 0.7812\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.7708 - val_loss: 0.4576 - val_accuracy: 0.7812\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7708 - val_loss: 0.4572 - val_accuracy: 0.7760\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7656 - val_loss: 0.4569 - val_accuracy: 0.7812\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7691 - val_loss: 0.4565 - val_accuracy: 0.7812\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.7708 - val_loss: 0.4561 - val_accuracy: 0.7865\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.7708 - val_loss: 0.4557 - val_accuracy: 0.7865\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.7743 - val_loss: 0.4553 - val_accuracy: 0.7865\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.7726 - val_loss: 0.4551 - val_accuracy: 0.7865\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.7691 - val_loss: 0.4548 - val_accuracy: 0.7865\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7708 - val_loss: 0.4547 - val_accuracy: 0.7865\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.7708 - val_loss: 0.4546 - val_accuracy: 0.7865\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7708 - val_loss: 0.4544 - val_accuracy: 0.7865\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.7691 - val_loss: 0.4541 - val_accuracy: 0.7865\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7743 - val_loss: 0.4542 - val_accuracy: 0.7917\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.7726 - val_loss: 0.4540 - val_accuracy: 0.7865\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.7743 - val_loss: 0.4539 - val_accuracy: 0.7917\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7795 - val_loss: 0.4536 - val_accuracy: 0.7917\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7708 - val_loss: 0.4532 - val_accuracy: 0.7917\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7691 - val_loss: 0.4529 - val_accuracy: 0.7865\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7795 - val_loss: 0.4527 - val_accuracy: 0.7917\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7778 - val_loss: 0.4527 - val_accuracy: 0.7917\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7743 - val_loss: 0.4525 - val_accuracy: 0.7917\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7812 - val_loss: 0.4523 - val_accuracy: 0.7917\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7778 - val_loss: 0.4521 - val_accuracy: 0.7917\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7778 - val_loss: 0.4518 - val_accuracy: 0.7917\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7778 - val_loss: 0.4515 - val_accuracy: 0.7917\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7760 - val_loss: 0.4513 - val_accuracy: 0.7917\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4583 - accuracy: 0.7760 - val_loss: 0.4514 - val_accuracy: 0.7917\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.7812 - val_loss: 0.4512 - val_accuracy: 0.7917\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4580 - accuracy: 0.7812 - val_loss: 0.4509 - val_accuracy: 0.7917\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4577 - accuracy: 0.7795 - val_loss: 0.4507 - val_accuracy: 0.7917\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4577 - accuracy: 0.7778 - val_loss: 0.4505 - val_accuracy: 0.7969\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.7743 - val_loss: 0.4502 - val_accuracy: 0.7917\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.7726 - val_loss: 0.4503 - val_accuracy: 0.7917\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7760 - val_loss: 0.4501 - val_accuracy: 0.7969\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.7778 - val_loss: 0.4498 - val_accuracy: 0.7969\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4563 - accuracy: 0.7743 - val_loss: 0.4497 - val_accuracy: 0.8021\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.7778 - val_loss: 0.4495 - val_accuracy: 0.7969\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4553 - accuracy: 0.7743 - val_loss: 0.4494 - val_accuracy: 0.8021\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.7743 - val_loss: 0.4494 - val_accuracy: 0.8021\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4554 - accuracy: 0.7760 - val_loss: 0.4493 - val_accuracy: 0.8021\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.7691 - val_loss: 0.4492 - val_accuracy: 0.8021\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4539 - accuracy: 0.7778 - val_loss: 0.4492 - val_accuracy: 0.7969\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.7743 - val_loss: 0.4488 - val_accuracy: 0.8021\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.7812 - val_loss: 0.4487 - val_accuracy: 0.8021\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7708 - val_loss: 0.4485 - val_accuracy: 0.8021\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4534 - accuracy: 0.7708 - val_loss: 0.4482 - val_accuracy: 0.8021\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.7795 - val_loss: 0.4480 - val_accuracy: 0.8021\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.7743 - val_loss: 0.4477 - val_accuracy: 0.8021\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7726 - val_loss: 0.4477 - val_accuracy: 0.8021\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.7708 - val_loss: 0.4476 - val_accuracy: 0.8021\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4524 - accuracy: 0.7726 - val_loss: 0.4476 - val_accuracy: 0.8021\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.7708 - val_loss: 0.4476 - val_accuracy: 0.8021\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.7656 - val_loss: 0.4474 - val_accuracy: 0.8021\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4520 - accuracy: 0.7743 - val_loss: 0.4474 - val_accuracy: 0.8021\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4521 - accuracy: 0.7691 - val_loss: 0.4474 - val_accuracy: 0.8021\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.7726 - val_loss: 0.4473 - val_accuracy: 0.8021\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.7708 - val_loss: 0.4475 - val_accuracy: 0.8073\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.7726 - val_loss: 0.4475 - val_accuracy: 0.8021\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.7743 - val_loss: 0.4472 - val_accuracy: 0.8073\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4514 - accuracy: 0.7760 - val_loss: 0.4472 - val_accuracy: 0.8073\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.7778 - val_loss: 0.4471 - val_accuracy: 0.8021\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.7726 - val_loss: 0.4469 - val_accuracy: 0.8021\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4510 - accuracy: 0.7726 - val_loss: 0.4469 - val_accuracy: 0.8021\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.7691 - val_loss: 0.4469 - val_accuracy: 0.8021\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4505 - accuracy: 0.7726 - val_loss: 0.4467 - val_accuracy: 0.8073\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.7760 - val_loss: 0.4464 - val_accuracy: 0.8073\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.7760 - val_loss: 0.4466 - val_accuracy: 0.8021\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4506 - accuracy: 0.7778 - val_loss: 0.4462 - val_accuracy: 0.8073\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.7726 - val_loss: 0.4464 - val_accuracy: 0.8073\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4501 - accuracy: 0.7760 - val_loss: 0.4462 - val_accuracy: 0.8073\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.7691 - val_loss: 0.4461 - val_accuracy: 0.8125\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.7743 - val_loss: 0.4459 - val_accuracy: 0.8125\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.7743 - val_loss: 0.4460 - val_accuracy: 0.8073\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7760 - val_loss: 0.4460 - val_accuracy: 0.8125\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.7708 - val_loss: 0.4461 - val_accuracy: 0.8073\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.7778 - val_loss: 0.4459 - val_accuracy: 0.8125\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4491 - accuracy: 0.7795 - val_loss: 0.4456 - val_accuracy: 0.8125\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4487 - accuracy: 0.7778 - val_loss: 0.4455 - val_accuracy: 0.8125\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.7812 - val_loss: 0.4454 - val_accuracy: 0.8125\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4486 - accuracy: 0.7795 - val_loss: 0.4454 - val_accuracy: 0.8125\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4485 - accuracy: 0.7760 - val_loss: 0.4454 - val_accuracy: 0.8125\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4487 - accuracy: 0.7795 - val_loss: 0.4451 - val_accuracy: 0.8125\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.7743 - val_loss: 0.4451 - val_accuracy: 0.8125\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4480 - accuracy: 0.7795 - val_loss: 0.4452 - val_accuracy: 0.8177\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4484 - accuracy: 0.7812 - val_loss: 0.4451 - val_accuracy: 0.8125\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.7760 - val_loss: 0.4452 - val_accuracy: 0.8177\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7760 - val_loss: 0.4451 - val_accuracy: 0.8177\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.7726 - val_loss: 0.4451 - val_accuracy: 0.8177\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.7830 - val_loss: 0.4450 - val_accuracy: 0.8125\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4476 - accuracy: 0.7743 - val_loss: 0.4450 - val_accuracy: 0.8125\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.7778 - val_loss: 0.4451 - val_accuracy: 0.8073\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.7760 - val_loss: 0.4450 - val_accuracy: 0.8125\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.7795 - val_loss: 0.4451 - val_accuracy: 0.8125\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4473 - accuracy: 0.7795 - val_loss: 0.4449 - val_accuracy: 0.8125\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.7743 - val_loss: 0.4448 - val_accuracy: 0.8073\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7830 - val_loss: 0.4448 - val_accuracy: 0.8073\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4467 - accuracy: 0.7812 - val_loss: 0.4448 - val_accuracy: 0.8073\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.7760 - val_loss: 0.4448 - val_accuracy: 0.8073\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.7812 - val_loss: 0.4448 - val_accuracy: 0.8073\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4464 - accuracy: 0.7812 - val_loss: 0.4447 - val_accuracy: 0.8125\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.7726 - val_loss: 0.4447 - val_accuracy: 0.8073\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.7795 - val_loss: 0.4446 - val_accuracy: 0.8073\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.7778 - val_loss: 0.4446 - val_accuracy: 0.8073\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.7795 - val_loss: 0.4444 - val_accuracy: 0.8125\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.7778 - val_loss: 0.4445 - val_accuracy: 0.8073\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7795 - val_loss: 0.4443 - val_accuracy: 0.8125\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.7778 - val_loss: 0.4446 - val_accuracy: 0.8073\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7812 - val_loss: 0.4445 - val_accuracy: 0.8073\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4451 - accuracy: 0.7795 - val_loss: 0.4446 - val_accuracy: 0.8125\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.7743 - val_loss: 0.4442 - val_accuracy: 0.8125\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.7795 - val_loss: 0.4443 - val_accuracy: 0.8073\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4452 - accuracy: 0.7795 - val_loss: 0.4441 - val_accuracy: 0.8073\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7778 - val_loss: 0.4441 - val_accuracy: 0.8073\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.7795 - val_loss: 0.4443 - val_accuracy: 0.8073\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.7760 - val_loss: 0.4443 - val_accuracy: 0.8073\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7778 - val_loss: 0.4444 - val_accuracy: 0.8073\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.7812 - val_loss: 0.4443 - val_accuracy: 0.8073\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.7812 - val_loss: 0.4443 - val_accuracy: 0.8073\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7847 - val_loss: 0.4444 - val_accuracy: 0.8073\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7812 - val_loss: 0.4441 - val_accuracy: 0.8073\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.7812 - val_loss: 0.4442 - val_accuracy: 0.8073\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7812 - val_loss: 0.4441 - val_accuracy: 0.8073\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.7760 - val_loss: 0.4442 - val_accuracy: 0.8073\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.7778 - val_loss: 0.4440 - val_accuracy: 0.8073\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.7760 - val_loss: 0.4440 - val_accuracy: 0.8073\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4435 - accuracy: 0.7795 - val_loss: 0.4440 - val_accuracy: 0.8073\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4438 - accuracy: 0.7778 - val_loss: 0.4438 - val_accuracy: 0.8073\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7778 - val_loss: 0.4437 - val_accuracy: 0.8073\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.7743 - val_loss: 0.4442 - val_accuracy: 0.8073\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7795 - val_loss: 0.4439 - val_accuracy: 0.8073\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7847 - val_loss: 0.4437 - val_accuracy: 0.8073\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.7726 - val_loss: 0.4439 - val_accuracy: 0.8073\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.7760 - val_loss: 0.4437 - val_accuracy: 0.8073\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.7812 - val_loss: 0.4437 - val_accuracy: 0.8073\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7795 - val_loss: 0.4437 - val_accuracy: 0.8073\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.7778 - val_loss: 0.4436 - val_accuracy: 0.8073\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4424 - accuracy: 0.7760 - val_loss: 0.4436 - val_accuracy: 0.8073\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.7795 - val_loss: 0.4435 - val_accuracy: 0.8073\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.7795 - val_loss: 0.4434 - val_accuracy: 0.8073\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7795 - val_loss: 0.4434 - val_accuracy: 0.8073\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.7778 - val_loss: 0.4433 - val_accuracy: 0.8073\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.7812 - val_loss: 0.4433 - val_accuracy: 0.8073\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.7795 - val_loss: 0.4434 - val_accuracy: 0.8073\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7760 - val_loss: 0.4436 - val_accuracy: 0.8073\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.7812 - val_loss: 0.4434 - val_accuracy: 0.8073\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.7778 - val_loss: 0.4434 - val_accuracy: 0.8073\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.7812 - val_loss: 0.4432 - val_accuracy: 0.8073\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4411 - accuracy: 0.7795 - val_loss: 0.4432 - val_accuracy: 0.8073\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.7847 - val_loss: 0.4433 - val_accuracy: 0.8073\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.7778 - val_loss: 0.4432 - val_accuracy: 0.8073\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.7795 - val_loss: 0.4434 - val_accuracy: 0.8073\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.7743 - val_loss: 0.4433 - val_accuracy: 0.8073\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.7812 - val_loss: 0.4432 - val_accuracy: 0.8073\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.7743 - val_loss: 0.4431 - val_accuracy: 0.8073\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.7743 - val_loss: 0.4432 - val_accuracy: 0.8073\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.7778 - val_loss: 0.4430 - val_accuracy: 0.8073\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.7812 - val_loss: 0.4430 - val_accuracy: 0.8073\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.7795 - val_loss: 0.4434 - val_accuracy: 0.8073\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.7812 - val_loss: 0.4434 - val_accuracy: 0.8073\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7812 - val_loss: 0.4430 - val_accuracy: 0.8073\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.7795 - val_loss: 0.4430 - val_accuracy: 0.8073\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7812 - val_loss: 0.4430 - val_accuracy: 0.8073\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7865 - val_loss: 0.4428 - val_accuracy: 0.8073\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4390 - accuracy: 0.7847 - val_loss: 0.4427 - val_accuracy: 0.8073\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4387 - accuracy: 0.7847 - val_loss: 0.4424 - val_accuracy: 0.8073\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.7778 - val_loss: 0.4428 - val_accuracy: 0.8073\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.7830 - val_loss: 0.4427 - val_accuracy: 0.8073\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4390 - accuracy: 0.7812 - val_loss: 0.4427 - val_accuracy: 0.8073\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.7760 - val_loss: 0.4425 - val_accuracy: 0.8073\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.7830 - val_loss: 0.4425 - val_accuracy: 0.8073\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.7778 - val_loss: 0.4425 - val_accuracy: 0.8073\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.7812 - val_loss: 0.4427 - val_accuracy: 0.8073\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.7795 - val_loss: 0.4425 - val_accuracy: 0.8073\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4376 - accuracy: 0.7795 - val_loss: 0.4424 - val_accuracy: 0.8073\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.7812 - val_loss: 0.4424 - val_accuracy: 0.8073\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.7795 - val_loss: 0.4425 - val_accuracy: 0.8073\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.7778 - val_loss: 0.4426 - val_accuracy: 0.8073\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.7795 - val_loss: 0.4426 - val_accuracy: 0.8073\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.7830 - val_loss: 0.4426 - val_accuracy: 0.8073\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.7795 - val_loss: 0.4425 - val_accuracy: 0.8073\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.7847 - val_loss: 0.4425 - val_accuracy: 0.8073\n",
      "accuracy is =  0.734375\n",
      "ROC is =  0.6527041357370096\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation_func=\"sigmoid\"\n",
    "optimizer_tec=RMSprop(learning_rate=0.003)\n",
    "model_1_x(activation_func,optimizer_tec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6445 - accuracy: 0.6545 - val_loss: 0.6346 - val_accuracy: 0.6406\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6160 - accuracy: 0.6545 - val_loss: 0.6063 - val_accuracy: 0.6406\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5957 - accuracy: 0.6493 - val_loss: 0.5841 - val_accuracy: 0.6562\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5773 - accuracy: 0.6667 - val_loss: 0.5652 - val_accuracy: 0.6771\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.6875 - val_loss: 0.5487 - val_accuracy: 0.6927\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5480 - accuracy: 0.7083 - val_loss: 0.5344 - val_accuracy: 0.7292\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.7309 - val_loss: 0.5221 - val_accuracy: 0.7604\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7309 - val_loss: 0.5115 - val_accuracy: 0.7552\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.7396 - val_loss: 0.5017 - val_accuracy: 0.7604\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7465 - val_loss: 0.4949 - val_accuracy: 0.7760\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7517 - val_loss: 0.4890 - val_accuracy: 0.7760\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7639 - val_loss: 0.4837 - val_accuracy: 0.7760\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.7656 - val_loss: 0.4797 - val_accuracy: 0.7760\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7726 - val_loss: 0.4761 - val_accuracy: 0.7812\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7656 - val_loss: 0.4734 - val_accuracy: 0.7760\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.7674 - val_loss: 0.4710 - val_accuracy: 0.7812\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7674 - val_loss: 0.4689 - val_accuracy: 0.7917\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.7674 - val_loss: 0.4672 - val_accuracy: 0.7917\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.7656 - val_loss: 0.4660 - val_accuracy: 0.7969\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7674 - val_loss: 0.4648 - val_accuracy: 0.8021\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.7691 - val_loss: 0.4639 - val_accuracy: 0.8021\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.7708 - val_loss: 0.4627 - val_accuracy: 0.7969\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7708 - val_loss: 0.4621 - val_accuracy: 0.8021\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7708 - val_loss: 0.4609 - val_accuracy: 0.8021\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.7708 - val_loss: 0.4607 - val_accuracy: 0.7969\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.7708 - val_loss: 0.4608 - val_accuracy: 0.7917\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.7691 - val_loss: 0.4592 - val_accuracy: 0.7917\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.7708 - val_loss: 0.4584 - val_accuracy: 0.7917\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7743 - val_loss: 0.4586 - val_accuracy: 0.7812\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.7726 - val_loss: 0.4581 - val_accuracy: 0.7865\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7726 - val_loss: 0.4575 - val_accuracy: 0.7865\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.7726 - val_loss: 0.4571 - val_accuracy: 0.7865\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7708 - val_loss: 0.4569 - val_accuracy: 0.7865\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.7708 - val_loss: 0.4567 - val_accuracy: 0.7812\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.7708 - val_loss: 0.4567 - val_accuracy: 0.7865\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7691 - val_loss: 0.4561 - val_accuracy: 0.7917\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 946us/step - loss: 0.4651 - accuracy: 0.7726 - val_loss: 0.4558 - val_accuracy: 0.7917\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.7726 - val_loss: 0.4554 - val_accuracy: 0.7917\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7743 - val_loss: 0.4547 - val_accuracy: 0.7917\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7726 - val_loss: 0.4544 - val_accuracy: 0.7917\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4633 - accuracy: 0.7726 - val_loss: 0.4543 - val_accuracy: 0.7917\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.7708 - val_loss: 0.4543 - val_accuracy: 0.7917\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7726 - val_loss: 0.4545 - val_accuracy: 0.7865\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7726 - val_loss: 0.4543 - val_accuracy: 0.7865\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.7778 - val_loss: 0.4539 - val_accuracy: 0.7917\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7726 - val_loss: 0.4536 - val_accuracy: 0.7865\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7778 - val_loss: 0.4532 - val_accuracy: 0.7917\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7726 - val_loss: 0.4531 - val_accuracy: 0.7917\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7708 - val_loss: 0.4533 - val_accuracy: 0.7865\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.7743 - val_loss: 0.4525 - val_accuracy: 0.7917\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.7778 - val_loss: 0.4524 - val_accuracy: 0.7917\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7778 - val_loss: 0.4528 - val_accuracy: 0.7917\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7726 - val_loss: 0.4525 - val_accuracy: 0.7917\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.7743 - val_loss: 0.4523 - val_accuracy: 0.7917\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.7778 - val_loss: 0.4523 - val_accuracy: 0.7917\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7743 - val_loss: 0.4521 - val_accuracy: 0.7969\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4586 - accuracy: 0.7743 - val_loss: 0.4518 - val_accuracy: 0.7917\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4579 - accuracy: 0.7708 - val_loss: 0.4522 - val_accuracy: 0.7917\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.7726 - val_loss: 0.4521 - val_accuracy: 0.7917\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4576 - accuracy: 0.7760 - val_loss: 0.4513 - val_accuracy: 0.7969\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4574 - accuracy: 0.7726 - val_loss: 0.4511 - val_accuracy: 0.7969\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.7708 - val_loss: 0.4520 - val_accuracy: 0.7917\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7726 - val_loss: 0.4516 - val_accuracy: 0.7969\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4563 - accuracy: 0.7708 - val_loss: 0.4514 - val_accuracy: 0.7969\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.7691 - val_loss: 0.4514 - val_accuracy: 0.7969\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.7708 - val_loss: 0.4512 - val_accuracy: 0.7969\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.7708 - val_loss: 0.4511 - val_accuracy: 0.7917\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.7743 - val_loss: 0.4519 - val_accuracy: 0.7917\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7726 - val_loss: 0.4513 - val_accuracy: 0.7969\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4553 - accuracy: 0.7674 - val_loss: 0.4505 - val_accuracy: 0.8021\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.7674 - val_loss: 0.4510 - val_accuracy: 0.7969\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4548 - accuracy: 0.7691 - val_loss: 0.4509 - val_accuracy: 0.8021\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.7674 - val_loss: 0.4512 - val_accuracy: 0.8021\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4548 - accuracy: 0.7726 - val_loss: 0.4506 - val_accuracy: 0.8021\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.7691 - val_loss: 0.4509 - val_accuracy: 0.8021\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.7691 - val_loss: 0.4510 - val_accuracy: 0.8021\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.7708 - val_loss: 0.4508 - val_accuracy: 0.8021\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.7708 - val_loss: 0.4506 - val_accuracy: 0.8021\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4533 - accuracy: 0.7691 - val_loss: 0.4510 - val_accuracy: 0.8021\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.7708 - val_loss: 0.4507 - val_accuracy: 0.7969\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.7691 - val_loss: 0.4501 - val_accuracy: 0.7969\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7726 - val_loss: 0.4499 - val_accuracy: 0.7969\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.7691 - val_loss: 0.4504 - val_accuracy: 0.8021\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.7708 - val_loss: 0.4506 - val_accuracy: 0.7969\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.7708 - val_loss: 0.4508 - val_accuracy: 0.8073\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4521 - accuracy: 0.7674 - val_loss: 0.4512 - val_accuracy: 0.8021\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7691 - val_loss: 0.4499 - val_accuracy: 0.8073\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7708 - val_loss: 0.4506 - val_accuracy: 0.8021\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.7708 - val_loss: 0.4503 - val_accuracy: 0.8073\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.7708 - val_loss: 0.4498 - val_accuracy: 0.8021\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.7726 - val_loss: 0.4500 - val_accuracy: 0.8021\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4510 - accuracy: 0.7743 - val_loss: 0.4497 - val_accuracy: 0.8021\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4510 - accuracy: 0.7726 - val_loss: 0.4499 - val_accuracy: 0.8073\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.7743 - val_loss: 0.4500 - val_accuracy: 0.8073\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.7743 - val_loss: 0.4507 - val_accuracy: 0.8125\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.7778 - val_loss: 0.4501 - val_accuracy: 0.8073\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.7743 - val_loss: 0.4494 - val_accuracy: 0.8073\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.7726 - val_loss: 0.4499 - val_accuracy: 0.8125\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.7708 - val_loss: 0.4501 - val_accuracy: 0.8125\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7726 - val_loss: 0.4499 - val_accuracy: 0.8073\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.7726 - val_loss: 0.4496 - val_accuracy: 0.8125\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.7726 - val_loss: 0.4498 - val_accuracy: 0.8125\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4491 - accuracy: 0.7726 - val_loss: 0.4494 - val_accuracy: 0.8125\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4492 - accuracy: 0.7760 - val_loss: 0.4502 - val_accuracy: 0.8125\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.7778 - val_loss: 0.4496 - val_accuracy: 0.8125\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.7778 - val_loss: 0.4499 - val_accuracy: 0.8125\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.7778 - val_loss: 0.4492 - val_accuracy: 0.8125\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.7743 - val_loss: 0.4493 - val_accuracy: 0.8177\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.7726 - val_loss: 0.4492 - val_accuracy: 0.8177\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.7726 - val_loss: 0.4495 - val_accuracy: 0.8125\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.7778 - val_loss: 0.4492 - val_accuracy: 0.8125\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.7760 - val_loss: 0.4492 - val_accuracy: 0.8177\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.7726 - val_loss: 0.4500 - val_accuracy: 0.8177\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.7778 - val_loss: 0.4495 - val_accuracy: 0.8177\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.7760 - val_loss: 0.4488 - val_accuracy: 0.8177\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.7760 - val_loss: 0.4488 - val_accuracy: 0.8229\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.7760 - val_loss: 0.4484 - val_accuracy: 0.8229\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7795 - val_loss: 0.4497 - val_accuracy: 0.8073\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7795 - val_loss: 0.4494 - val_accuracy: 0.8073\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.7760 - val_loss: 0.4487 - val_accuracy: 0.8177\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4463 - accuracy: 0.7778 - val_loss: 0.4493 - val_accuracy: 0.8229\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.7812 - val_loss: 0.4493 - val_accuracy: 0.8177\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.7812 - val_loss: 0.4488 - val_accuracy: 0.8125\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7812 - val_loss: 0.4497 - val_accuracy: 0.8073\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.7847 - val_loss: 0.4489 - val_accuracy: 0.8073\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7795 - val_loss: 0.4488 - val_accuracy: 0.8177\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.7778 - val_loss: 0.4488 - val_accuracy: 0.8177\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.7726 - val_loss: 0.4491 - val_accuracy: 0.8229\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.7812 - val_loss: 0.4488 - val_accuracy: 0.8125\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4450 - accuracy: 0.7795 - val_loss: 0.4487 - val_accuracy: 0.8177\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.7778 - val_loss: 0.4489 - val_accuracy: 0.8073\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.7812 - val_loss: 0.4487 - val_accuracy: 0.8177\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7812 - val_loss: 0.4488 - val_accuracy: 0.8177\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7795 - val_loss: 0.4481 - val_accuracy: 0.8177\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.7812 - val_loss: 0.4482 - val_accuracy: 0.8021\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.7812 - val_loss: 0.4484 - val_accuracy: 0.8125\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7812 - val_loss: 0.4483 - val_accuracy: 0.8177\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.7795 - val_loss: 0.4492 - val_accuracy: 0.8125\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7795 - val_loss: 0.4489 - val_accuracy: 0.8177\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7812 - val_loss: 0.4481 - val_accuracy: 0.8125\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7795 - val_loss: 0.4484 - val_accuracy: 0.8125\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4428 - accuracy: 0.7795 - val_loss: 0.4484 - val_accuracy: 0.8125\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.7812 - val_loss: 0.4483 - val_accuracy: 0.8125\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.7795 - val_loss: 0.4483 - val_accuracy: 0.8125\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.7795 - val_loss: 0.4483 - val_accuracy: 0.8125\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.7830 - val_loss: 0.4478 - val_accuracy: 0.8125\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.7812 - val_loss: 0.4478 - val_accuracy: 0.8177\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.7812 - val_loss: 0.4485 - val_accuracy: 0.8125\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.7830 - val_loss: 0.4478 - val_accuracy: 0.8125\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7847 - val_loss: 0.4478 - val_accuracy: 0.8125\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7812 - val_loss: 0.4479 - val_accuracy: 0.8125\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.7847 - val_loss: 0.4483 - val_accuracy: 0.8125\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.7778 - val_loss: 0.4482 - val_accuracy: 0.8125\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.7778 - val_loss: 0.4484 - val_accuracy: 0.8021\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7812 - val_loss: 0.4474 - val_accuracy: 0.8125\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.7812 - val_loss: 0.4478 - val_accuracy: 0.8125\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.7812 - val_loss: 0.4476 - val_accuracy: 0.8125\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.7830 - val_loss: 0.4476 - val_accuracy: 0.8125\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.7795 - val_loss: 0.4476 - val_accuracy: 0.8125\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.7847 - val_loss: 0.4470 - val_accuracy: 0.8125\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.7812 - val_loss: 0.4475 - val_accuracy: 0.8125\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7830 - val_loss: 0.4472 - val_accuracy: 0.8125\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7795 - val_loss: 0.4474 - val_accuracy: 0.8125\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7830 - val_loss: 0.4473 - val_accuracy: 0.8073\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.7778 - val_loss: 0.4479 - val_accuracy: 0.7969\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4390 - accuracy: 0.7795 - val_loss: 0.4473 - val_accuracy: 0.8125\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4390 - accuracy: 0.7812 - val_loss: 0.4471 - val_accuracy: 0.8125\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4390 - accuracy: 0.7830 - val_loss: 0.4474 - val_accuracy: 0.8125\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4389 - accuracy: 0.7830 - val_loss: 0.4473 - val_accuracy: 0.8073\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.7812 - val_loss: 0.4471 - val_accuracy: 0.8073\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.7847 - val_loss: 0.4467 - val_accuracy: 0.8125\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.7812 - val_loss: 0.4469 - val_accuracy: 0.8073\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.7830 - val_loss: 0.4464 - val_accuracy: 0.8073\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.7847 - val_loss: 0.4468 - val_accuracy: 0.8125\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.7865 - val_loss: 0.4468 - val_accuracy: 0.8125\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.7830 - val_loss: 0.4465 - val_accuracy: 0.8125\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.7812 - val_loss: 0.4470 - val_accuracy: 0.8073\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.7812 - val_loss: 0.4460 - val_accuracy: 0.8073\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.7847 - val_loss: 0.4462 - val_accuracy: 0.8125\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.7847 - val_loss: 0.4462 - val_accuracy: 0.8125\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7865 - val_loss: 0.4473 - val_accuracy: 0.8073\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4365 - accuracy: 0.7847 - val_loss: 0.4460 - val_accuracy: 0.8073\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.7847 - val_loss: 0.4455 - val_accuracy: 0.8125\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.7865 - val_loss: 0.4459 - val_accuracy: 0.8073\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.7865 - val_loss: 0.4464 - val_accuracy: 0.8125\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.7865 - val_loss: 0.4457 - val_accuracy: 0.8125\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.7847 - val_loss: 0.4466 - val_accuracy: 0.8073\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.7847 - val_loss: 0.4461 - val_accuracy: 0.8073\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.7830 - val_loss: 0.4458 - val_accuracy: 0.8125\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4352 - accuracy: 0.7865 - val_loss: 0.4464 - val_accuracy: 0.8177\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.7865 - val_loss: 0.4463 - val_accuracy: 0.8073\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.7847 - val_loss: 0.4457 - val_accuracy: 0.8125\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.7847 - val_loss: 0.4460 - val_accuracy: 0.8125\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.7847 - val_loss: 0.4453 - val_accuracy: 0.8177\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.7882 - val_loss: 0.4460 - val_accuracy: 0.8177\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.7865 - val_loss: 0.4457 - val_accuracy: 0.8177\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.7865 - val_loss: 0.4457 - val_accuracy: 0.8177\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.7882 - val_loss: 0.4458 - val_accuracy: 0.8177\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4336 - accuracy: 0.7847 - val_loss: 0.4455 - val_accuracy: 0.8177\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.7865 - val_loss: 0.4458 - val_accuracy: 0.8125\n",
      "accuracy is =  0.734375\n",
      "ROC is =  0.6527041357370096\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation_func=\"sigmoid\"\n",
    "optimizer_tec=Adam(learning_rate=0.003)\n",
    "model_1_x(activation_func,optimizer_tec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the optimization technique give same accuracy 0.734, but the traning and validation loss sharply reudce more in RMSprop and Adam as compare to SDG and Adagrade at same epoch. \n",
    "\n",
    "the other we observe in RMSprop and Adam is they make the model overfitt. you can in RMSprop technique validation loss is more than traning loss after 175th epoch.and in Adam this overfitt start at 125th epoch. if trainig loss continue reduce but validatio loss is not than its mean model is no more gernalize. it cannot perform well on gernal input outside the trainig data.\n",
    "\n",
    "the over fitting problem can be solve by using regularization techniques for example **Dropout** , **Early stopping** or ** Regularization penalty in cost function** . there many more that you can explore, learn and play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now in cell below, the activation function change will from sigmoid to ReLU and we use the Adam otmization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.6133 - accuracy: 0.6806 - val_loss: 0.5720 - val_accuracy: 0.7500\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5705 - accuracy: 0.7222 - val_loss: 0.5352 - val_accuracy: 0.7760\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5401 - accuracy: 0.7413 - val_loss: 0.5075 - val_accuracy: 0.7865\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.7622 - val_loss: 0.4917 - val_accuracy: 0.7812\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7604 - val_loss: 0.4779 - val_accuracy: 0.7865\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.7622 - val_loss: 0.4693 - val_accuracy: 0.7812\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7604 - val_loss: 0.4653 - val_accuracy: 0.7865\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7726 - val_loss: 0.4620 - val_accuracy: 0.7917\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.7743 - val_loss: 0.4621 - val_accuracy: 0.7917\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4680 - accuracy: 0.7743 - val_loss: 0.4595 - val_accuracy: 0.7917\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.7760 - val_loss: 0.4585 - val_accuracy: 0.7865\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.7743 - val_loss: 0.4575 - val_accuracy: 0.7865\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7795 - val_loss: 0.4572 - val_accuracy: 0.7865\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7795 - val_loss: 0.4575 - val_accuracy: 0.7865\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7812 - val_loss: 0.4556 - val_accuracy: 0.7865\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4584 - accuracy: 0.7795 - val_loss: 0.4561 - val_accuracy: 0.7865\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7830 - val_loss: 0.4549 - val_accuracy: 0.7969\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4553 - accuracy: 0.7812 - val_loss: 0.4562 - val_accuracy: 0.7917\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.7812 - val_loss: 0.4556 - val_accuracy: 0.7917\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.7812 - val_loss: 0.4548 - val_accuracy: 0.7969\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.7830 - val_loss: 0.4563 - val_accuracy: 0.7969\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4518 - accuracy: 0.7812 - val_loss: 0.4548 - val_accuracy: 0.7969\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4515 - accuracy: 0.7778 - val_loss: 0.4558 - val_accuracy: 0.7969\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.7760 - val_loss: 0.4545 - val_accuracy: 0.7969\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4505 - accuracy: 0.7812 - val_loss: 0.4556 - val_accuracy: 0.7969\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4500 - accuracy: 0.7760 - val_loss: 0.4552 - val_accuracy: 0.7969\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7830 - val_loss: 0.4563 - val_accuracy: 0.7969\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7795 - val_loss: 0.4566 - val_accuracy: 0.7969\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.7795 - val_loss: 0.4564 - val_accuracy: 0.7917\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7778 - val_loss: 0.4547 - val_accuracy: 0.7969\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4468 - accuracy: 0.7760 - val_loss: 0.4550 - val_accuracy: 0.7969\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.7830 - val_loss: 0.4551 - val_accuracy: 0.7969\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7795 - val_loss: 0.4562 - val_accuracy: 0.7969\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7795 - val_loss: 0.4551 - val_accuracy: 0.7917\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7778 - val_loss: 0.4566 - val_accuracy: 0.7969\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.7778 - val_loss: 0.4553 - val_accuracy: 0.7969\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.7778 - val_loss: 0.4559 - val_accuracy: 0.7969\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.7760 - val_loss: 0.4547 - val_accuracy: 0.7969\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.7743 - val_loss: 0.4545 - val_accuracy: 0.7969\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.7795 - val_loss: 0.4556 - val_accuracy: 0.7969\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.7760 - val_loss: 0.4543 - val_accuracy: 0.7969\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.7778 - val_loss: 0.4539 - val_accuracy: 0.7969\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.7778 - val_loss: 0.4539 - val_accuracy: 0.7969\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.7726 - val_loss: 0.4520 - val_accuracy: 0.8021\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7760 - val_loss: 0.4543 - val_accuracy: 0.7969\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.7760 - val_loss: 0.4522 - val_accuracy: 0.7969\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.7812 - val_loss: 0.4543 - val_accuracy: 0.7969\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.7830 - val_loss: 0.4538 - val_accuracy: 0.7969\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7778 - val_loss: 0.4515 - val_accuracy: 0.7969\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.7830 - val_loss: 0.4542 - val_accuracy: 0.7969\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.7778 - val_loss: 0.4519 - val_accuracy: 0.8021\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.7795 - val_loss: 0.4529 - val_accuracy: 0.8021\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.7830 - val_loss: 0.4518 - val_accuracy: 0.7969\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7812 - val_loss: 0.4535 - val_accuracy: 0.7969\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.7778 - val_loss: 0.4519 - val_accuracy: 0.7969\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.7865 - val_loss: 0.4508 - val_accuracy: 0.8073\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.7830 - val_loss: 0.4533 - val_accuracy: 0.7969\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.7882 - val_loss: 0.4521 - val_accuracy: 0.8021\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.7865 - val_loss: 0.4517 - val_accuracy: 0.7969\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.7847 - val_loss: 0.4521 - val_accuracy: 0.7969\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4384 - accuracy: 0.7743 - val_loss: 0.4535 - val_accuracy: 0.7969\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.7812 - val_loss: 0.4519 - val_accuracy: 0.8073\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.7830 - val_loss: 0.4508 - val_accuracy: 0.8073\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.7847 - val_loss: 0.4505 - val_accuracy: 0.8073\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.7882 - val_loss: 0.4522 - val_accuracy: 0.7969\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.7899 - val_loss: 0.4513 - val_accuracy: 0.8073\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.7812 - val_loss: 0.4506 - val_accuracy: 0.8125\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.7830 - val_loss: 0.4507 - val_accuracy: 0.8125\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.7899 - val_loss: 0.4519 - val_accuracy: 0.8021\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.7899 - val_loss: 0.4504 - val_accuracy: 0.8021\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4322 - accuracy: 0.7882 - val_loss: 0.4516 - val_accuracy: 0.8125\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4330 - accuracy: 0.7847 - val_loss: 0.4512 - val_accuracy: 0.8073\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.7934 - val_loss: 0.4534 - val_accuracy: 0.7917\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.7917 - val_loss: 0.4520 - val_accuracy: 0.8073\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.7899 - val_loss: 0.4518 - val_accuracy: 0.8073\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.7847 - val_loss: 0.4521 - val_accuracy: 0.8073\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.7830 - val_loss: 0.4509 - val_accuracy: 0.8073\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.3962 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4293 - accuracy: 0.7830 - val_loss: 0.4506 - val_accuracy: 0.8125\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4300 - accuracy: 0.7865 - val_loss: 0.4526 - val_accuracy: 0.8073\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4291 - accuracy: 0.7812 - val_loss: 0.4503 - val_accuracy: 0.8073\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.7865 - val_loss: 0.4502 - val_accuracy: 0.8125\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.7865 - val_loss: 0.4524 - val_accuracy: 0.8073\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.7882 - val_loss: 0.4530 - val_accuracy: 0.8073\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.7865 - val_loss: 0.4516 - val_accuracy: 0.8073\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.7899 - val_loss: 0.4523 - val_accuracy: 0.8073\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4289 - accuracy: 0.7778 - val_loss: 0.4523 - val_accuracy: 0.8073\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.7865 - val_loss: 0.4526 - val_accuracy: 0.8073\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.7830 - val_loss: 0.4523 - val_accuracy: 0.8073\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.7865 - val_loss: 0.4520 - val_accuracy: 0.8073\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.7830 - val_loss: 0.4530 - val_accuracy: 0.8073\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.7899 - val_loss: 0.4542 - val_accuracy: 0.8073\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.7847 - val_loss: 0.4522 - val_accuracy: 0.8125\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4265 - accuracy: 0.7899 - val_loss: 0.4540 - val_accuracy: 0.8073\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.7847 - val_loss: 0.4518 - val_accuracy: 0.8073\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4270 - accuracy: 0.7830 - val_loss: 0.4533 - val_accuracy: 0.8125\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.7899 - val_loss: 0.4542 - val_accuracy: 0.8125\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.7899 - val_loss: 0.4530 - val_accuracy: 0.8125\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.7830 - val_loss: 0.4528 - val_accuracy: 0.8125\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.7830 - val_loss: 0.4546 - val_accuracy: 0.8073\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.7917 - val_loss: 0.4524 - val_accuracy: 0.8073\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.7847 - val_loss: 0.4542 - val_accuracy: 0.8125\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.7865 - val_loss: 0.4554 - val_accuracy: 0.8125\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.7917 - val_loss: 0.4552 - val_accuracy: 0.8073\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.7882 - val_loss: 0.4548 - val_accuracy: 0.8073\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.7917 - val_loss: 0.4540 - val_accuracy: 0.8125\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.7934 - val_loss: 0.4549 - val_accuracy: 0.8073\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4241 - accuracy: 0.7899 - val_loss: 0.4536 - val_accuracy: 0.8125\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.7899 - val_loss: 0.4538 - val_accuracy: 0.8125\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4243 - accuracy: 0.7917 - val_loss: 0.4541 - val_accuracy: 0.8125\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.7882 - val_loss: 0.4563 - val_accuracy: 0.8073\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.7969 - val_loss: 0.4550 - val_accuracy: 0.8021\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.7882 - val_loss: 0.4537 - val_accuracy: 0.8125\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4229 - accuracy: 0.7934 - val_loss: 0.4545 - val_accuracy: 0.8125\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.7934 - val_loss: 0.4556 - val_accuracy: 0.8073\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4228 - accuracy: 0.7917 - val_loss: 0.4550 - val_accuracy: 0.8073\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.7830 - val_loss: 0.4548 - val_accuracy: 0.8073\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.7899 - val_loss: 0.4544 - val_accuracy: 0.8073\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.7865 - val_loss: 0.4551 - val_accuracy: 0.8073\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.7951 - val_loss: 0.4563 - val_accuracy: 0.8021\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.7969 - val_loss: 0.4553 - val_accuracy: 0.8125\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.7986 - val_loss: 0.4552 - val_accuracy: 0.8073\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.7951 - val_loss: 0.4547 - val_accuracy: 0.8073\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4214 - accuracy: 0.7899 - val_loss: 0.4560 - val_accuracy: 0.7969\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4210 - accuracy: 0.7934 - val_loss: 0.4583 - val_accuracy: 0.8021\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8003 - val_loss: 0.4566 - val_accuracy: 0.8073\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.7969 - val_loss: 0.4532 - val_accuracy: 0.8125\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4208 - accuracy: 0.7986 - val_loss: 0.4569 - val_accuracy: 0.8021\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4212 - accuracy: 0.7986 - val_loss: 0.4574 - val_accuracy: 0.8021\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8003 - val_loss: 0.4574 - val_accuracy: 0.8021\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.7969 - val_loss: 0.4549 - val_accuracy: 0.8073\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4212 - accuracy: 0.8003 - val_loss: 0.4556 - val_accuracy: 0.8073\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4198 - accuracy: 0.7951 - val_loss: 0.4594 - val_accuracy: 0.7969\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.7986 - val_loss: 0.4574 - val_accuracy: 0.8021\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.7951 - val_loss: 0.4558 - val_accuracy: 0.8021\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.8003 - val_loss: 0.4578 - val_accuracy: 0.8021\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.8003 - val_loss: 0.4581 - val_accuracy: 0.8021\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4198 - accuracy: 0.7986 - val_loss: 0.4575 - val_accuracy: 0.8073\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8003 - val_loss: 0.4557 - val_accuracy: 0.8073\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4186 - accuracy: 0.7969 - val_loss: 0.4557 - val_accuracy: 0.8021\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4182 - accuracy: 0.7969 - val_loss: 0.4575 - val_accuracy: 0.8125\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8056 - val_loss: 0.4606 - val_accuracy: 0.8021\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8038 - val_loss: 0.4600 - val_accuracy: 0.7969\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8090 - val_loss: 0.4566 - val_accuracy: 0.8021\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4185 - accuracy: 0.7969 - val_loss: 0.4571 - val_accuracy: 0.8021\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.7969 - val_loss: 0.4593 - val_accuracy: 0.7969\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4179 - accuracy: 0.8003 - val_loss: 0.4583 - val_accuracy: 0.7917\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4184 - accuracy: 0.8021 - val_loss: 0.4569 - val_accuracy: 0.8021\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.7934 - val_loss: 0.4590 - val_accuracy: 0.8073\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8003 - val_loss: 0.4595 - val_accuracy: 0.7969\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4166 - accuracy: 0.8056 - val_loss: 0.4565 - val_accuracy: 0.8021\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4167 - accuracy: 0.8003 - val_loss: 0.4568 - val_accuracy: 0.8073\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4157 - accuracy: 0.8021 - val_loss: 0.4575 - val_accuracy: 0.7969\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4167 - accuracy: 0.8090 - val_loss: 0.4584 - val_accuracy: 0.8073\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.7986 - val_loss: 0.4556 - val_accuracy: 0.8021\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4159 - accuracy: 0.8038 - val_loss: 0.4582 - val_accuracy: 0.7969\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8056 - val_loss: 0.4586 - val_accuracy: 0.7917\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.7986 - val_loss: 0.4574 - val_accuracy: 0.8021\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8056 - val_loss: 0.4568 - val_accuracy: 0.7969\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4155 - accuracy: 0.8038 - val_loss: 0.4586 - val_accuracy: 0.8021\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8073 - val_loss: 0.4571 - val_accuracy: 0.8021\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4150 - accuracy: 0.8021 - val_loss: 0.4584 - val_accuracy: 0.7917\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8056 - val_loss: 0.4583 - val_accuracy: 0.7969\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8073 - val_loss: 0.4575 - val_accuracy: 0.7969\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.7934 - val_loss: 0.4558 - val_accuracy: 0.8073\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8108 - val_loss: 0.4580 - val_accuracy: 0.8021\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8038 - val_loss: 0.4610 - val_accuracy: 0.7969\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8056 - val_loss: 0.4548 - val_accuracy: 0.8073\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8038 - val_loss: 0.4546 - val_accuracy: 0.7969\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8021 - val_loss: 0.4591 - val_accuracy: 0.7917\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8021 - val_loss: 0.4581 - val_accuracy: 0.7969\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8108 - val_loss: 0.4568 - val_accuracy: 0.7917\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8073 - val_loss: 0.4560 - val_accuracy: 0.7969\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.8056 - val_loss: 0.4574 - val_accuracy: 0.8021\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.8038 - val_loss: 0.4565 - val_accuracy: 0.7969\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8021 - val_loss: 0.4560 - val_accuracy: 0.7917\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.8073 - val_loss: 0.4582 - val_accuracy: 0.7969\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4118 - accuracy: 0.8038 - val_loss: 0.4573 - val_accuracy: 0.7969\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.8073 - val_loss: 0.4576 - val_accuracy: 0.7969\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.8108 - val_loss: 0.4579 - val_accuracy: 0.8021\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4112 - accuracy: 0.8073 - val_loss: 0.4565 - val_accuracy: 0.7969\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4134 - accuracy: 0.8125 - val_loss: 0.4572 - val_accuracy: 0.7917\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8073 - val_loss: 0.4576 - val_accuracy: 0.7865\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8073 - val_loss: 0.4593 - val_accuracy: 0.7917\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8090 - val_loss: 0.4581 - val_accuracy: 0.7865\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8056 - val_loss: 0.4583 - val_accuracy: 0.7917\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4112 - accuracy: 0.8108 - val_loss: 0.4561 - val_accuracy: 0.7969\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8090 - val_loss: 0.4572 - val_accuracy: 0.7969\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8038 - val_loss: 0.4574 - val_accuracy: 0.8073\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.3921 - accuracy: 0.81 - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8108 - val_loss: 0.4569 - val_accuracy: 0.7969\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8142 - val_loss: 0.4569 - val_accuracy: 0.7917\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8056 - val_loss: 0.4593 - val_accuracy: 0.7917\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8142 - val_loss: 0.4567 - val_accuracy: 0.7865\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8073 - val_loss: 0.4575 - val_accuracy: 0.7917\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8021 - val_loss: 0.4578 - val_accuracy: 0.7865\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4095 - accuracy: 0.8160 - val_loss: 0.4588 - val_accuracy: 0.8021\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8090 - val_loss: 0.4577 - val_accuracy: 0.7865\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8125 - val_loss: 0.4585 - val_accuracy: 0.7917\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8090 - val_loss: 0.4565 - val_accuracy: 0.8021\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4086 - accuracy: 0.8056 - val_loss: 0.4568 - val_accuracy: 0.8021\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8056 - val_loss: 0.4590 - val_accuracy: 0.7969\n",
      "accuracy is =  0.734375\n",
      "ROC is =  0.6527041357370096\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation_func=\"relu\"\n",
    "optimizer_tec=Adam(learning_rate=0.003)\n",
    "model_1_x(activation_func,optimizer_tec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,by changing the acctivation function to ReLU, model get overfitt after almost 20th epoch and it also redcue to loss 0.45 at almost 10th epoch, after this validation loss get saturate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "So far we use same network and observe changes due to chaning its parameters like Optimization techniques and Activation function.\n",
    "Now, we change the network.\n",
    "first we built network with one hidden layer of 12 neurons, 8 input and single out layer\n",
    "Now, in following cell we will build a **Deep Neural Network** with 2 hidden layers of 6 neurons, 8 input and single out layer.\n",
    "Note: a network with 2 or more hidden layer is called deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2_x(activation_function,optimizer_technique): # in this function activation function and optimization technique will\n",
    "                                                        #be pass as function argument\n",
    "    model=Sequential()\n",
    "    model.add(Dense(6,input_shape=(8,),activation=activation_function)) #hidden layer 1 with 12 neuron, \n",
    "              #   and input layer with 8 neuron as 8 feature column we have in dataset\n",
    "    model.add(Dense(6, activation=activation_function))     \n",
    "    model.add(Dense(1, activation='sigmoid')) # output layer  only 1 neuron (one output, binary 0 or 1) \n",
    "\n",
    "    model.compile(optimizer=optimizer_technique,loss=\"binary_crossentropy\",metrics=[\"accuracy\"]);\n",
    "    develop_model_history=model.fit(x_train_normalize,y_train,validation_data=(x_test_normalize,y_test), epochs=200);\n",
    "    \n",
    "    y_predcit_from_model=model.predict_classes(x_test_normalize)\n",
    "    \n",
    "    model_accuracy=accuracy_score(y_test,y_predcit_from_model)\n",
    "    model_roc=roc_auc_score(y_test,y_predcit_from_model)\n",
    "    print(\"accuracy is = \",model_1_accuracy)\n",
    "    print(\"ROC is = \",model_1_roc)\n",
    "    \n",
    "    plt.plot(develop_model_history.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "    plt.plot(develop_model_history.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.6971 - accuracy: 0.5104 - val_loss: 0.6643 - val_accuracy: 0.5885\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.6652 - accuracy: 0.6267 - val_loss: 0.6394 - val_accuracy: 0.6302\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.6580 - val_loss: 0.6158 - val_accuracy: 0.6458\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6150 - accuracy: 0.6684 - val_loss: 0.5872 - val_accuracy: 0.6719\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5841 - accuracy: 0.6858 - val_loss: 0.5525 - val_accuracy: 0.6875\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5487 - accuracy: 0.7066 - val_loss: 0.5167 - val_accuracy: 0.7500\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7257 - val_loss: 0.4858 - val_accuracy: 0.7760\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7656 - val_loss: 0.4674 - val_accuracy: 0.7917\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7674 - val_loss: 0.4573 - val_accuracy: 0.7969\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4763 - accuracy: 0.7656 - val_loss: 0.4555 - val_accuracy: 0.7969\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.7639 - val_loss: 0.4525 - val_accuracy: 0.7917\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7674 - val_loss: 0.4523 - val_accuracy: 0.8073\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.7726 - val_loss: 0.4503 - val_accuracy: 0.7969\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4670 - accuracy: 0.7691 - val_loss: 0.4494 - val_accuracy: 0.7917\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.7778 - val_loss: 0.4488 - val_accuracy: 0.8021\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7760 - val_loss: 0.4479 - val_accuracy: 0.7969\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.7691 - val_loss: 0.4465 - val_accuracy: 0.7969\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7760 - val_loss: 0.4465 - val_accuracy: 0.7865\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4599 - accuracy: 0.7760 - val_loss: 0.4448 - val_accuracy: 0.7865\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7726 - val_loss: 0.4444 - val_accuracy: 0.8021\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4577 - accuracy: 0.7726 - val_loss: 0.4435 - val_accuracy: 0.7969\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4577 - accuracy: 0.7795 - val_loss: 0.4439 - val_accuracy: 0.7917\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.7743 - val_loss: 0.4440 - val_accuracy: 0.7917\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7743 - val_loss: 0.4433 - val_accuracy: 0.7917\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.7830 - val_loss: 0.4438 - val_accuracy: 0.7917\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.7812 - val_loss: 0.4443 - val_accuracy: 0.7917\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.7812 - val_loss: 0.4435 - val_accuracy: 0.7917\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7795 - val_loss: 0.4443 - val_accuracy: 0.7917\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.7812 - val_loss: 0.4448 - val_accuracy: 0.7917\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4518 - accuracy: 0.7795 - val_loss: 0.4445 - val_accuracy: 0.7917\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.7830 - val_loss: 0.4453 - val_accuracy: 0.7917\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4506 - accuracy: 0.7812 - val_loss: 0.4451 - val_accuracy: 0.7917\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7847 - val_loss: 0.4457 - val_accuracy: 0.7917\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.7830 - val_loss: 0.4462 - val_accuracy: 0.7917\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4486 - accuracy: 0.7812 - val_loss: 0.4469 - val_accuracy: 0.7917\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.7795 - val_loss: 0.4470 - val_accuracy: 0.7917\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.7847 - val_loss: 0.4469 - val_accuracy: 0.7917\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.7847 - val_loss: 0.4468 - val_accuracy: 0.7917\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7847 - val_loss: 0.4469 - val_accuracy: 0.7917\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.7812 - val_loss: 0.4474 - val_accuracy: 0.7917\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4465 - accuracy: 0.7830 - val_loss: 0.4472 - val_accuracy: 0.7917\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4457 - accuracy: 0.7830 - val_loss: 0.4473 - val_accuracy: 0.7917\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4449 - accuracy: 0.7812 - val_loss: 0.4476 - val_accuracy: 0.7917\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7847 - val_loss: 0.4468 - val_accuracy: 0.7917\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.7865 - val_loss: 0.4473 - val_accuracy: 0.7917\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.7812 - val_loss: 0.4481 - val_accuracy: 0.7917\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7812 - val_loss: 0.4461 - val_accuracy: 0.7917\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.7882 - val_loss: 0.4473 - val_accuracy: 0.7917\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.7865 - val_loss: 0.4467 - val_accuracy: 0.7917\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7865 - val_loss: 0.4484 - val_accuracy: 0.7917\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7847 - val_loss: 0.4468 - val_accuracy: 0.7917\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4428 - accuracy: 0.7865 - val_loss: 0.4469 - val_accuracy: 0.7917\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.7865 - val_loss: 0.4479 - val_accuracy: 0.7917\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.7847 - val_loss: 0.4464 - val_accuracy: 0.7917\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.7847 - val_loss: 0.4478 - val_accuracy: 0.7917\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.7865 - val_loss: 0.4483 - val_accuracy: 0.7917\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.7830 - val_loss: 0.4489 - val_accuracy: 0.7917\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.7865 - val_loss: 0.4487 - val_accuracy: 0.7917\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.7882 - val_loss: 0.4493 - val_accuracy: 0.7917\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7882 - val_loss: 0.4500 - val_accuracy: 0.7865\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.7882 - val_loss: 0.4491 - val_accuracy: 0.7917\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.7917 - val_loss: 0.4503 - val_accuracy: 0.7917\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.7917 - val_loss: 0.4492 - val_accuracy: 0.7917\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.7882 - val_loss: 0.4503 - val_accuracy: 0.7917\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4402 - accuracy: 0.7899 - val_loss: 0.4517 - val_accuracy: 0.7865\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.7899 - val_loss: 0.4497 - val_accuracy: 0.7917\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7882 - val_loss: 0.4512 - val_accuracy: 0.7917\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.7882 - val_loss: 0.4516 - val_accuracy: 0.7865\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.7899 - val_loss: 0.4511 - val_accuracy: 0.7917\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7882 - val_loss: 0.4532 - val_accuracy: 0.7865\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.7882 - val_loss: 0.4516 - val_accuracy: 0.7812\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4390 - accuracy: 0.7899 - val_loss: 0.4518 - val_accuracy: 0.7865\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4380 - accuracy: 0.7934 - val_loss: 0.4520 - val_accuracy: 0.7865\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.7917 - val_loss: 0.4520 - val_accuracy: 0.7969\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.7917 - val_loss: 0.4523 - val_accuracy: 0.7865\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.7899 - val_loss: 0.4522 - val_accuracy: 0.7865\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.7917 - val_loss: 0.4529 - val_accuracy: 0.7865\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.7951 - val_loss: 0.4527 - val_accuracy: 0.7865\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.7934 - val_loss: 0.4542 - val_accuracy: 0.7917\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.7899 - val_loss: 0.4521 - val_accuracy: 0.7865\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.7899 - val_loss: 0.4520 - val_accuracy: 0.7917\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4367 - accuracy: 0.7882 - val_loss: 0.4532 - val_accuracy: 0.7865\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.7917 - val_loss: 0.4521 - val_accuracy: 0.7865\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.7899 - val_loss: 0.4527 - val_accuracy: 0.7865\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.7899 - val_loss: 0.4530 - val_accuracy: 0.7865\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.7951 - val_loss: 0.4527 - val_accuracy: 0.7865\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4348 - accuracy: 0.7934 - val_loss: 0.4528 - val_accuracy: 0.7865\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.7917 - val_loss: 0.4525 - val_accuracy: 0.7865\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.7917 - val_loss: 0.4523 - val_accuracy: 0.7812\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4357 - accuracy: 0.7865 - val_loss: 0.4537 - val_accuracy: 0.7812\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.7934 - val_loss: 0.4521 - val_accuracy: 0.7865\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.7899 - val_loss: 0.4521 - val_accuracy: 0.7812\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.7951 - val_loss: 0.4525 - val_accuracy: 0.7865\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.7969 - val_loss: 0.4514 - val_accuracy: 0.7812\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.7986 - val_loss: 0.4509 - val_accuracy: 0.7812\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.7917 - val_loss: 0.4514 - val_accuracy: 0.7865\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4337 - accuracy: 0.7934 - val_loss: 0.4503 - val_accuracy: 0.7812\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.8003 - val_loss: 0.4522 - val_accuracy: 0.7865\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.7934 - val_loss: 0.4505 - val_accuracy: 0.7865\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.7899 - val_loss: 0.4493 - val_accuracy: 0.7865\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4306 - accuracy: 0.8021 - val_loss: 0.4498 - val_accuracy: 0.7865\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.7934 - val_loss: 0.4497 - val_accuracy: 0.7865\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.7969 - val_loss: 0.4486 - val_accuracy: 0.7865\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.7986 - val_loss: 0.4505 - val_accuracy: 0.7865\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.7969 - val_loss: 0.4484 - val_accuracy: 0.7865\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4289 - accuracy: 0.8003 - val_loss: 0.4487 - val_accuracy: 0.7865\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4284 - accuracy: 0.8003 - val_loss: 0.4485 - val_accuracy: 0.7917\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4281 - accuracy: 0.7986 - val_loss: 0.4490 - val_accuracy: 0.7865\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4281 - accuracy: 0.8021 - val_loss: 0.4473 - val_accuracy: 0.7917\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4291 - accuracy: 0.7986 - val_loss: 0.4500 - val_accuracy: 0.7865\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.8073 - val_loss: 0.4506 - val_accuracy: 0.7917\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.7969 - val_loss: 0.4504 - val_accuracy: 0.7865\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8056 - val_loss: 0.4499 - val_accuracy: 0.7917\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4260 - accuracy: 0.7969 - val_loss: 0.4488 - val_accuracy: 0.7865\n",
      "Epoch 115/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4256 - accuracy: 0.8038 - val_loss: 0.4494 - val_accuracy: 0.7917\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8003 - val_loss: 0.4513 - val_accuracy: 0.7865\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4248 - accuracy: 0.7969 - val_loss: 0.4489 - val_accuracy: 0.7917\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4249 - accuracy: 0.8038 - val_loss: 0.4488 - val_accuracy: 0.7812\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.7986 - val_loss: 0.4494 - val_accuracy: 0.7917\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.7986 - val_loss: 0.4503 - val_accuracy: 0.7865\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.8038 - val_loss: 0.4493 - val_accuracy: 0.7760\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.78 - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8021 - val_loss: 0.4516 - val_accuracy: 0.7917\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4232 - accuracy: 0.8038 - val_loss: 0.4508 - val_accuracy: 0.7760\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.8073 - val_loss: 0.4491 - val_accuracy: 0.7812\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.8003 - val_loss: 0.4513 - val_accuracy: 0.7812\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.8003 - val_loss: 0.4514 - val_accuracy: 0.7865\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.8108 - val_loss: 0.4513 - val_accuracy: 0.7865\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8056 - val_loss: 0.4515 - val_accuracy: 0.7708\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8038 - val_loss: 0.4543 - val_accuracy: 0.7760\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4212 - accuracy: 0.7917 - val_loss: 0.4529 - val_accuracy: 0.7812\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8021 - val_loss: 0.4515 - val_accuracy: 0.7812\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8108 - val_loss: 0.4519 - val_accuracy: 0.7812\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4212 - accuracy: 0.8056 - val_loss: 0.4554 - val_accuracy: 0.7760\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.7969 - val_loss: 0.4521 - val_accuracy: 0.7812\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8038 - val_loss: 0.4511 - val_accuracy: 0.7812\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8056 - val_loss: 0.4543 - val_accuracy: 0.7760\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4192 - accuracy: 0.8073 - val_loss: 0.4533 - val_accuracy: 0.7812\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.8038 - val_loss: 0.4531 - val_accuracy: 0.7812\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.8003 - val_loss: 0.4559 - val_accuracy: 0.7708\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8038 - val_loss: 0.4532 - val_accuracy: 0.7812\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8038 - val_loss: 0.4540 - val_accuracy: 0.7812\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.71 - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8003 - val_loss: 0.4561 - val_accuracy: 0.7760\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8021 - val_loss: 0.4556 - val_accuracy: 0.7812\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4184 - accuracy: 0.8073 - val_loss: 0.4583 - val_accuracy: 0.7760\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8056 - val_loss: 0.4571 - val_accuracy: 0.7760\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8038 - val_loss: 0.4580 - val_accuracy: 0.7760\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.8056 - val_loss: 0.4585 - val_accuracy: 0.7760\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4139 - accuracy: 0.8073 - val_loss: 0.4571 - val_accuracy: 0.7760\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.8003 - val_loss: 0.4593 - val_accuracy: 0.7760\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4136 - accuracy: 0.8038 - val_loss: 0.4596 - val_accuracy: 0.7760\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8090 - val_loss: 0.4583 - val_accuracy: 0.7760\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4154 - accuracy: 0.8021 - val_loss: 0.4625 - val_accuracy: 0.7708\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4141 - accuracy: 0.8038 - val_loss: 0.4583 - val_accuracy: 0.7760\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8003 - val_loss: 0.4628 - val_accuracy: 0.7656\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8003 - val_loss: 0.4606 - val_accuracy: 0.7760\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8056 - val_loss: 0.4609 - val_accuracy: 0.7760\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8021 - val_loss: 0.4626 - val_accuracy: 0.7656\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8073 - val_loss: 0.4631 - val_accuracy: 0.7656\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4126 - accuracy: 0.8090 - val_loss: 0.4649 - val_accuracy: 0.7708\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.84 - 0s 3ms/step - loss: 0.4109 - accuracy: 0.8003 - val_loss: 0.4651 - val_accuracy: 0.7708\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.7917 - val_loss: 0.4633 - val_accuracy: 0.7656\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.8073 - val_loss: 0.4639 - val_accuracy: 0.7604\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8090 - val_loss: 0.4665 - val_accuracy: 0.7708\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.7951 - val_loss: 0.4651 - val_accuracy: 0.7604\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4109 - accuracy: 0.8073 - val_loss: 0.4647 - val_accuracy: 0.7604\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4116 - accuracy: 0.8003 - val_loss: 0.4692 - val_accuracy: 0.7604\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8038 - val_loss: 0.4661 - val_accuracy: 0.7604\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.7986 - val_loss: 0.4680 - val_accuracy: 0.7604\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.7986 - val_loss: 0.4666 - val_accuracy: 0.7604\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8003 - val_loss: 0.4683 - val_accuracy: 0.7552\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8021 - val_loss: 0.4690 - val_accuracy: 0.7656\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.75 - 0s 2ms/step - loss: 0.4077 - accuracy: 0.8056 - val_loss: 0.4678 - val_accuracy: 0.7604\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4069 - accuracy: 0.8056 - val_loss: 0.4692 - val_accuracy: 0.7552\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8056 - val_loss: 0.4688 - val_accuracy: 0.7604\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8073 - val_loss: 0.4691 - val_accuracy: 0.7604\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8038 - val_loss: 0.4715 - val_accuracy: 0.7604\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8003 - val_loss: 0.4728 - val_accuracy: 0.7604\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8073 - val_loss: 0.4710 - val_accuracy: 0.7552\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8021 - val_loss: 0.4729 - val_accuracy: 0.7552\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8056 - val_loss: 0.4733 - val_accuracy: 0.7500\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.8073 - val_loss: 0.4740 - val_accuracy: 0.7604\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8038 - val_loss: 0.4727 - val_accuracy: 0.7604\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8038 - val_loss: 0.4752 - val_accuracy: 0.7604\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8038 - val_loss: 0.4768 - val_accuracy: 0.7604\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8021 - val_loss: 0.4737 - val_accuracy: 0.7552\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.8003 - val_loss: 0.4774 - val_accuracy: 0.7604\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8038 - val_loss: 0.4761 - val_accuracy: 0.7500\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8108 - val_loss: 0.4753 - val_accuracy: 0.7604\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.8056 - val_loss: 0.4771 - val_accuracy: 0.7604\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.8125 - val_loss: 0.4799 - val_accuracy: 0.7604\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4050 - accuracy: 0.8021 - val_loss: 0.4798 - val_accuracy: 0.7708\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.7986 - val_loss: 0.4783 - val_accuracy: 0.7656\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8056 - val_loss: 0.4802 - val_accuracy: 0.7656\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.4044 - accuracy: 0.8038 - val_loss: 0.4819 - val_accuracy: 0.7604\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8003 - val_loss: 0.4816 - val_accuracy: 0.7604\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4019 - accuracy: 0.8090 - val_loss: 0.4808 - val_accuracy: 0.7604\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8056 - val_loss: 0.4808 - val_accuracy: 0.7708\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8056 - val_loss: 0.4850 - val_accuracy: 0.7656\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8108 - val_loss: 0.4818 - val_accuracy: 0.7604\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8056 - val_loss: 0.4858 - val_accuracy: 0.7604\n",
      "accuracy is =  0.734375\n",
      "ROC is =  0.6527041357370096\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "activation_func=\"relu\"\n",
    "optimizer_tec=Adam(learning_rate=0.003)\n",
    "model_2_x(activation_func,optimizer_tec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
